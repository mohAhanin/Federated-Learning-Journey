{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from scipy.stats import ks_2samp, chi2_contingency\n",
    "import concurrent.futures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "       253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "         0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.11764706, 0.14117647,\n",
       "       0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,\n",
       "       0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,\n",
       "       0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model creation function\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We have to convert our keras model to tff model\n",
    "  This is necessary because TFF operates on its own set of model interfaces that are designed to be compatible with federated computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a federated learning setting, the model needs to be serialized and sent to multiple clients, where it will be used to compute updates locally.\n",
    "These updates are then sent back to the server where they are aggregated. \n",
    "This process requires the model to have certain properties and methods that are not present in a standard Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Keras model to TFF model\n",
    "def model_fn():\n",
    "    keras_model = create_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model,\n",
    "        input_spec=client_datasets[0].element_spec,\n",
    "        \n",
    "        #means that the input specification is based on the first client dataset. This assumes that all client datasets have the same format\n",
    "        #used to specify the type and shape of the data that the model expects for its inputs\n",
    "        #The element_spec property of a tf.data.Dataset object gives the shape and type information of the dataset elements\n",
    "        \n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "    \n",
    "# The tff.learning.from_keras_model function takes a Keras model and wraps it in a TFF model interface,\n",
    "# which includes methods for sending the model to clients,\n",
    "# computing updates, and aggregating updates on the server.\n",
    "# It also includes methods for training, evaluation, and prediction that can be used in a federated setting.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of clients in the federated learning system\n",
    "num_clients = 2\n",
    "\n",
    "# Initialize an empty list to hold the datasets for each client\n",
    "client_datasets = []\n",
    "\n",
    "# Create a dataset for each client\n",
    "for _ in range(num_clients):\n",
    "    # Generate 50 random 28x28 images (values between 0 and 1)\n",
    "    images = np.random.rand(50, 28, 28)\n",
    "    \n",
    "    # Generate 50 random labels (integers between 0 and 9)\n",
    "    labels = np.random.randint(0, 10, 50)\n",
    "    \n",
    "    # Create a dataset from the images and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    \n",
    "    # Batch the dataset into groups of 10\n",
    "    batched_dataset = dataset.batch(10)\n",
    "    #.batch(10): This batches the dataset into batches of 10 elements each.\n",
    "    # This means that during training, the model will be updated based on 10 examples at a time.\n",
    "    \n",
    "    # Add the batched dataset to the list of client datasets\n",
    "    client_datasets.append(batched_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy client datasets (reduced size)\n",
    "# num_clients = 10\n",
    "# client_datasets = [tf.data.Dataset.from_tensor_slices((np.random.rand(50, 28, 28), np.random.randint(0, 10, 50))).batch(10) for _ in range(num_clients)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part of the code is creating a list of federated learning algorithms, one for each client in the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create federated learning algorithms for each client\n",
    "federated_algorithms = [tff.learning.build_federated_averaging_process(\n",
    "    # This function is used to build a Federated Averaging process in TensorFlow Federated\n",
    "    \n",
    "    model_fn,\n",
    "    # means that this model will be used for the federated learning process\n",
    "    \n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "    # This is a function that returns an optimizer for updating the model parameters on the clients\n",
    "    \n",
    "    \n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0)\n",
    "    # This is a function that returns an optimizer for updating the global model parameters on the server\n",
    "    \n",
    ") for _ in range(num_clients)]\n",
    "\n",
    "\n",
    "len(federated_algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Way to Initialize the federated algorithm asynchronously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part of the code is a way to initialize a federated learning algorithm asynchronously, i.e., without blocking the rest of your program. This can be useful if the initialization process is time-consuming and you want to do other things while it’s happening. However, in this specific case, the code immediately waits for the result after submitting the function, so it doesn’t actually take advantage of the asynchronous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: ServerState(model=ModelWeights(trainable=[array([[ 0.05836407,  0.05039725, -0.04177264, ..., -0.04224399,\n",
      "         0.0736633 ,  0.06586737],\n",
      "       [ 0.05985995, -0.06860409,  0.074465  , ..., -0.0594831 ,\n",
      "        -0.03691043, -0.04610123],\n",
      "       [-0.0575145 , -0.0635508 , -0.00300542, ...,  0.03533187,\n",
      "         0.00281896, -0.01152197],\n",
      "       ...,\n",
      "       [ 0.05009299, -0.07541757, -0.02664017, ...,  0.02485169,\n",
      "         0.02423824, -0.01720573],\n",
      "       [-0.06613451, -0.0512032 , -0.0610115 , ...,  0.06814782,\n",
      "        -0.0532575 , -0.07847326],\n",
      "       [-0.04616087,  0.01027688, -0.03871288, ...,  0.06158916,\n",
      "        -0.03632533, -0.02753006]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[ 0.09079228, -0.20142454,  0.03336744, ...,  0.19803064,\n",
      "        -0.1271351 ,  0.11471353],\n",
      "       [ 0.18815203,  0.0476936 , -0.17116003, ...,  0.18208201,\n",
      "         0.11499403, -0.14835088],\n",
      "       [ 0.08127166,  0.12917112, -0.02250151, ..., -0.07717352,\n",
      "         0.09762083, -0.16370332],\n",
      "       ...,\n",
      "       [ 0.13797046,  0.02787532,  0.10038589, ...,  0.14792036,\n",
      "         0.07957973, -0.0635639 ],\n",
      "       [-0.19718137, -0.03731506,  0.05471252, ...,  0.15960889,\n",
      "         0.2046792 , -0.12749876],\n",
      "       [-0.10982961,  0.11243097, -0.0409631 , ..., -0.13773063,\n",
      "         0.05194451, -0.10309669]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)], non_trainable=[]), optimizer_state=[0], delta_aggregate_state=OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())]), model_broadcast_state=())\n"
     ]
    }
   ],
   "source": [
    "def initialize_algorithm(algorithm):\n",
    "    tff.backends.native.set_local_execution_context()\n",
    "    return algorithm.initialize()\n",
    "\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(initialize_algorithm, federated_algorithms[0])\n",
    "    state = future.result()\n",
    "\n",
    "print('Initial state:', state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model: This is the initial model weights. It’s divided into trainable and non_trainable weights. The trainable weights are the ones that the algorithm will update during training. In this case, they are initialized with random values.\n",
    "\n",
    "optimizer_state: This is the state of the optimizer. It can include things like the current step number, momentum variables for optimizers like Adam or RMSProp, etc. In this case, it’s just the current step number, initialized to 0.\n",
    "\n",
    "delta_aggregate_state and model_broadcast_state: These are used by the server to aggregate the model updates from the clients (delta_aggregate_state) and broadcast the updated model back to the clients (model_broadcast_state). In this case, they are both empty because no updates have been performed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the training in a separate event loop\n",
    "def run_training():\n",
    "    \n",
    "    tff.backends.native.set_local_execution_context()\n",
    "    # This sets up the execution context for TensorFlow Federated (TFF) in the local machine\n",
    "\n",
    "    states = [algorithm.initialize() for algorithm in federated_algorithms]\n",
    "    # This initializes the state of each federated learning algorithm\n",
    "    # The state includes the model parameters and any other variables that the algorithm needs to keep track of\n",
    "\n",
    "    # Train the models on the clients' data (reduced to 2 rounds)\n",
    "    for round_num in range(1, 3):\n",
    "        print(f\"Round {round_num}\")\n",
    "        for client_id in range(num_clients):\n",
    "            states[client_id], _ = federated_algorithms[client_id].next(states[client_id], [client_datasets[client_id]])\n",
    "\n",
    "        # Perform differential testing for each pair of clients\n",
    "        for i in range(num_clients):\n",
    "            for j in range(i+1, num_clients):\n",
    "                # Get the model weights after training\n",
    "                weights_i = states[i].model.trainable\n",
    "                weights_j = states[j].model.trainable\n",
    "\n",
    "                # Evaluate both models on train and test data\n",
    "                train_predictions_i, train_labels = evaluate_model_on_data(weights_i, create_model, X_train[:1000], Y_train[:1000])\n",
    "                test_predictions_i, test_labels = evaluate_model_on_data(weights_i, create_model, X_test[:1000], Y_test[:1000])\n",
    "                train_predictions_j, _ = evaluate_model_on_data(weights_j, create_model, X_train[:1000], Y_train[:1000])\n",
    "                test_predictions_j, _ = evaluate_model_on_data(weights_j, create_model, X_test[:1000], Y_test[:1000])\n",
    "\n",
    "                # Perform differential testing\n",
    "                print(f\"Comparison between Client {i} and Client {j}:\")\n",
    "                perform_differential_testing(train_predictions_i, train_predictions_j, train_labels, \"Train\")\n",
    "                perform_differential_testing(test_predictions_i, test_predictions_j, test_labels, \"Test\")\n",
    "                print()\n",
    "\n",
    "        # Average the predictions after differential testing\n",
    "        average_predictions_train = np.mean([evaluate_model_on_data(state.model.trainable, create_model, X_train[:1000], Y_train[:1000])[0] for state in states], axis=0)\n",
    "        average_predictions_test = np.mean([evaluate_model_on_data(state.model.trainable, create_model, X_test[:1000], Y_test[:1000])[0] for state in states], axis=0)\n",
    "\n",
    "        print(\"Average predictions on Train Data:\")\n",
    "        print(average_predictions_train)\n",
    "        print(\"Average predictions on Test Data:\")\n",
    "        print(average_predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on data\n",
    "def evaluate_model_on_data(weights, create_model_fn, X, Y):\n",
    "    model = create_model_fn()\n",
    "    model.set_weights(weights)\n",
    "    predictions = model.predict(X)\n",
    "    return predictions, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform differential testing\n",
    "def perform_differential_testing(predictions_i, predictions_j, labels, data_type):\n",
    "    # Criterion 1: Absolute differences between classes\n",
    "    pred_class_i = np.argmax(predictions_i, axis=1)\n",
    "    pred_class_j = np.argmax(predictions_j, axis=1)\n",
    "    Δ_class = np.sum(pred_class_i != pred_class_j)\n",
    "\n",
    "    # Criterion 2: Absolute differences between scores\n",
    "    Δ_score = np.sum(predictions_i != predictions_j)\n",
    "\n",
    "    # Criterion 3: Significance of difference between scores\n",
    "    P_KS = ks_2samp(predictions_i.flatten(), predictions_j.flatten()).pvalue\n",
    "\n",
    "    # Criterion 4: Significance of difference between classifications\n",
    "    contingency = np.array([[np.sum((pred_class_i == k) & (pred_class_j == l)) for l in range(10)] for k in range(10)])\n",
    "    contingency += 1  # Add-one smoothing\n",
    "    P_X2 = chi2_contingency(contingency)[1]\n",
    "\n",
    "    print(f\"{data_type} Data:\")\n",
    "    print(f\"Δ_class: {Δ_class}\")\n",
    "    print(f\"Δ_score: {Δ_score:.2f}\")\n",
    "    print(f\"P_KS: {P_KS:.4f}\")\n",
    "    print(f\"P_X2: {P_X2:.4f}\")\n",
    "    \n",
    "    if P_KS < 0.05 or P_X2 < 0.05:\n",
    "        print(\"Warning: Significant difference detected (p-value < 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1\n",
      "Comparison between Client 0 and Client 1:\n",
      "Train Data:\n",
      "Δ_class: 980\n",
      "Δ_score: 10000.00\n",
      "P_KS: 0.0000\n",
      "P_X2: 0.0000\n",
      "Warning: Significant difference detected (p-value < 0.05)\n",
      "Test Data:\n",
      "Δ_class: 975\n",
      "Δ_score: 10000.00\n",
      "P_KS: 0.0000\n",
      "P_X2: 0.0000\n",
      "Warning: Significant difference detected (p-value < 0.05)\n",
      "\n",
      "Average predictions on Train Data:\n",
      "[[0.05633319 0.14268357 0.09179597 ... 0.1419983  0.07862744 0.08928493]\n",
      " [0.07029216 0.11421728 0.12545967 ... 0.11117096 0.05623373 0.08167334]\n",
      " [0.09671271 0.1045723  0.13536033 ... 0.08277664 0.09316494 0.10946634]\n",
      " ...\n",
      " [0.05059628 0.16004598 0.11543691 ... 0.08459924 0.06276383 0.07659651]\n",
      " [0.0657655  0.14488554 0.10034356 ... 0.08232808 0.04641693 0.07233499]\n",
      " [0.06407616 0.16336723 0.08270907 ... 0.08378872 0.083827   0.07262444]]\n",
      "Average predictions on Test Data:\n",
      "[[0.06192347 0.12433006 0.13117662 ... 0.12946817 0.09776523 0.0848494 ]\n",
      " [0.07834484 0.11880942 0.10034712 ... 0.16962367 0.05097678 0.08333739]\n",
      " [0.08361384 0.09398962 0.12373546 ... 0.08991857 0.10353357 0.10349587]\n",
      " ...\n",
      " [0.063291   0.19096759 0.1065729  ... 0.09190824 0.04512028 0.11240079]\n",
      " [0.0666514  0.16782136 0.1329735  ... 0.12469662 0.05174188 0.07518487]\n",
      " [0.06820013 0.1225618  0.09678681 ... 0.10440136 0.07347501 0.1071007 ]]\n",
      "Round 2\n",
      "Comparison between Client 0 and Client 1:\n",
      "Train Data:\n",
      "Δ_class: 980\n",
      "Δ_score: 10000.00\n",
      "P_KS: 0.0000\n",
      "P_X2: 0.0000\n",
      "Warning: Significant difference detected (p-value < 0.05)\n",
      "Test Data:\n",
      "Δ_class: 971\n",
      "Δ_score: 10000.00\n",
      "P_KS: 0.0000\n",
      "P_X2: 0.0000\n",
      "Warning: Significant difference detected (p-value < 0.05)\n",
      "\n",
      "Average predictions on Train Data:\n",
      "[[0.05416049 0.14895388 0.09502463 ... 0.13950258 0.07927603 0.09011003]\n",
      " [0.06827398 0.11792386 0.12952578 ... 0.10883397 0.05594564 0.08186898]\n",
      " [0.09560217 0.10714559 0.13881432 ... 0.08099853 0.09329442 0.11037204]\n",
      " ...\n",
      " [0.04937493 0.16498494 0.11892234 ... 0.08147499 0.06236944 0.0753749 ]\n",
      " [0.06426368 0.14961673 0.10293119 ... 0.08156511 0.04682247 0.07313582]\n",
      " [0.06225121 0.16773728 0.08568924 ... 0.08134227 0.08405271 0.07259767]]\n",
      "Average predictions on Test Data:\n",
      "[[0.0606005  0.12733129 0.1339373  ... 0.12793474 0.09768519 0.08520261]\n",
      " [0.0763749  0.12290522 0.10338878 ... 0.16638394 0.05067601 0.08466169]\n",
      " [0.08322331 0.09480909 0.12541291 ... 0.08899048 0.10343456 0.10340513]\n",
      " ...\n",
      " [0.06048357 0.20233442 0.10984029 ... 0.08857556 0.04481328 0.11142125]\n",
      " [0.06331915 0.17671078 0.14042673 ... 0.11939007 0.0520076  0.07577495]\n",
      " [0.06735183 0.12413034 0.09977485 ... 0.10243361 0.07374056 0.10754385]]\n"
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    future = executor.submit(run_training)\n",
    "    future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_fed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
