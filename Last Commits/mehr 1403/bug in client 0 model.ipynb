{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1236e65f-8191-41df-bfd5-0d7b31a5a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import tensorflow_federated as tff\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "tff.backends.native.set_local_execution_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576b1b50-780a-4ba4-a73c-8a5687e86740",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_m, y_train_m), (x_test_m, y_test_m) = mnist.load_data()\n",
    "\n",
    "x_train_m = x_train_m.astype('float32') / 255.0\n",
    "x_test_m = x_test_m.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "x_train_m = x_train_m.reshape((x_train_m.shape[0], -1))  \n",
    "x_test_m = x_test_m.reshape((x_test_m.shape[0], -1))    \n",
    "\n",
    "y_train_m = tf.keras.utils.to_categorical(y_train_m, 10)\n",
    "y_test_m = tf.keras.utils.to_categorical(y_test_m, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7baa848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_m.shape, y_test_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d26f98a-9a4d-4a0e-a52d-ca606295cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mnist_mlp_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))     \n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))     \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6510fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_m.shape, x_test_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ed1662-76c4-4c7c-b77b-69c7704ab3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.8008\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2696 - accuracy: 0.9170\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.1683 - accuracy: 0.9498\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1356 - accuracy: 0.9587\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.0954 - accuracy: 0.9668\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.7945\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.2820 - accuracy: 0.9172\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1857 - accuracy: 0.9420\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1484 - accuracy: 0.9537\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1082 - accuracy: 0.9660\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.7930\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.9195\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1921 - accuracy: 0.9370\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1418 - accuracy: 0.9548\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1054 - accuracy: 0.9652\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.7967\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.9215\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1896 - accuracy: 0.9405\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1371 - accuracy: 0.9572\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.0952 - accuracy: 0.9702\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.6764 - accuracy: 0.7875\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.2922 - accuracy: 0.9125\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1991 - accuracy: 0.9370\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1681 - accuracy: 0.9492\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1167 - accuracy: 0.9635\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6323 - accuracy: 0.8030\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.2756 - accuracy: 0.9147\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.1852 - accuracy: 0.9408\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1386 - accuracy: 0.9578\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.1065 - accuracy: 0.9652\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6617 - accuracy: 0.7972\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2812 - accuracy: 0.9150\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1874 - accuracy: 0.9415\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1482 - accuracy: 0.9537\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.1102 - accuracy: 0.9668\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6405 - accuracy: 0.7930\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.2785 - accuracy: 0.9128\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1903 - accuracy: 0.9398\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.1391 - accuracy: 0.9573\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1265 - accuracy: 0.9587\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.7858\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2872 - accuracy: 0.9138\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.2006 - accuracy: 0.9390\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1414 - accuracy: 0.9577\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1148 - accuracy: 0.9625\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.6416 - accuracy: 0.7952\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.9217\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1826 - accuracy: 0.9455\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.1300 - accuracy: 0.9570\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1134 - accuracy: 0.9652\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 1.8036 - accuracy: 0.3897\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.9033 - accuracy: 0.6797\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6658 - accuracy: 0.7895\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.5362 - accuracy: 0.8313\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.4690 - accuracy: 0.8522\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6377 - accuracy: 0.7990\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2764 - accuracy: 0.9182\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2066 - accuracy: 0.9343\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1406 - accuracy: 0.9587\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1165 - accuracy: 0.9608\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6371 - accuracy: 0.7960\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.2745 - accuracy: 0.9147\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.1981 - accuracy: 0.9373\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1410 - accuracy: 0.9570\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1156 - accuracy: 0.9635\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6480 - accuracy: 0.7935\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.9160\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1826 - accuracy: 0.9438\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1379 - accuracy: 0.9572\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1029 - accuracy: 0.9685\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6580 - accuracy: 0.7935\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2901 - accuracy: 0.9127\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1919 - accuracy: 0.9382\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1615 - accuracy: 0.9488\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1215 - accuracy: 0.9622\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.7962\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.9190\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1885 - accuracy: 0.9402\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1329 - accuracy: 0.9553\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1062 - accuracy: 0.9653\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6571 - accuracy: 0.7958\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2849 - accuracy: 0.9135\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1870 - accuracy: 0.9437\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1479 - accuracy: 0.9535\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1075 - accuracy: 0.9657\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.6464 - accuracy: 0.7917\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.9135\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1852 - accuracy: 0.9423\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1401 - accuracy: 0.9538\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1145 - accuracy: 0.9602\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.6453 - accuracy: 0.7960\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.9148\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1944 - accuracy: 0.9375\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 6ms/step - loss: 0.1454 - accuracy: 0.9555\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1149 - accuracy: 0.9635\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.6522 - accuracy: 0.7908\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.9183\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1922 - accuracy: 0.9402\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1476 - accuracy: 0.9527\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 1s 5ms/step - loss: 0.1080 - accuracy: 0.9652\n"
     ]
    }
   ],
   "source": [
    "def split_data(x, y, num_splits):\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split_indices = np.array_split(indices, num_splits)\n",
    "    split_data = [(x[indices], y[indices]) for indices in split_indices]  \n",
    "    return split_data\n",
    "    \n",
    "num_models = 10\n",
    "client_data_m = split_data(x_train_m, y_train_m, num_models)\n",
    "\n",
    "accuracy_no_bug = []  # دقت برای حالت بدون نرمال‌سازی دوباره\n",
    "accuracy_with_bug = []  # دقت برای حالت با نرمال‌سازی دوباره برای کلاینت 0\n",
    "client_models_m = []  # لیست برای ذخیره مدل‌های هر کلاینت\n",
    "\n",
    "# حالت بدون نرمال‌سازی دوباره\n",
    "for i in range(len(client_data_m)):\n",
    "    model_data_m = client_data_m[i]\n",
    "    model_mnist = create_mnist_mlp_model()\n",
    "    \n",
    "    # آموزش مدل\n",
    "    model_mnist.fit(model_data_m[0], model_data_m[1], epochs=5, batch_size=32, verbose=1)\n",
    "    client_models_m.append(model_mnist)\n",
    "    \n",
    "    # ارزیابی دقت مدل بر روی داده‌های تست\n",
    "    loss, accuracy = model_mnist.evaluate(x_test_m, y_test_m, verbose=0)\n",
    "    accuracy_no_bug.append(accuracy)  # ذخیره دقت برای حالت بدون نرمال‌سازی دوباره\n",
    "\n",
    "# حالا، برای آموزش با نرمال‌سازی دوباره برای کلاینت 0\n",
    "client_models_m = []  # لیست برای ذخیره مدل‌های هر کلاینت (دوباره خالی می‌شود)\n",
    "for i in range(len(client_data_m)):\n",
    "    model_data_m = client_data_m[i]\n",
    "    model_mnist = create_mnist_mlp_model()\n",
    "    if i in [0]: \n",
    "        model_data_m = (model_data_m[0] / 255.0, model_data_m[1])\n",
    "    \n",
    "    # آموزش مدل\n",
    "    model_mnist.fit(model_data_m[0], model_data_m[1], epochs=5, batch_size=32, verbose=1)\n",
    "    client_models_m.append(model_mnist)\n",
    "    \n",
    "    # ارزیابی دقت مدل بر روی داده‌های تست\n",
    "    loss, accuracy = model_mnist.evaluate(x_test_m, y_test_m, verbose=0)\n",
    "    accuracy_with_bug.append(accuracy)  # ذخیره دقت برای حالت با نرمال‌سازی دوباره"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f391c",
   "metadata": {},
   "source": [
    "## Delta class function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7d2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance Matrix:\n",
      "[[   0. 2087. 2029. 2001. 1931. 1868. 2010. 1975. 1973. 1927.]\n",
      " [2087.    0.  649.  675.  628.  667.  544.  587.  543.  549.]\n",
      " [2029.  649.    0.  605.  616.  644.  613.  617.  600.  560.]\n",
      " [2001.  675.  605.    0.  516.  590.  587.  583.  548.  549.]\n",
      " [1931.  628.  616.  516.    0.  467.  499.  467.  474.  485.]\n",
      " [1868.  667.  644.  590.  467.    0.  532.  506.  528.  480.]\n",
      " [2010.  544.  613.  587.  499.  532.    0.  490.  464.  504.]\n",
      " [1975.  587.  617.  583.  467.  506.  490.    0.  469.  484.]\n",
      " [1973.  543.  600.  548.  474.  528.  464.  469.    0.  469.]\n",
      " [1927.  549.  560.  549.  485.  480.  504.  484.  469.    0.]]\n"
     ]
    }
   ],
   "source": [
    "def delta_class(model1, model2, x_test, y_test):\n",
    "    # Make predictions on the test data\n",
    "    predictions1 = np.argmax(model1.predict(x_test), axis=1)\n",
    "    predictions2 = np.argmax(model2.predict(x_test), axis=1)\n",
    "    \n",
    "    # Count the number of differing predictions\n",
    "    count = np.sum(predictions1 != predictions2)\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Initialize the distance matrix\n",
    "num_clients = len(client_models_m)\n",
    "distance_matrix = np.zeros((num_clients, num_clients))\n",
    "\n",
    "# Calculate the delta class for each pair of models\n",
    "for i in range(num_clients):\n",
    "    for j in range(i + 1, num_clients):\n",
    "        distance_matrix[i, j] = delta_class(client_models_m[i], client_models_m[j], x_test_m, y_test_m)\n",
    "        distance_matrix[j, i] = distance_matrix[i, j]  # Symmetric matrix\n",
    "\n",
    "# Print the distance matrix\n",
    "print(\"Distance Matrix:\")\n",
    "print(distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f997c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meancal(matrix):\n",
    "    temp = 0\n",
    "    x = matrix.shape[0]    \n",
    "    arrmean = []\n",
    "    \n",
    "    for i in range(0,x):\n",
    "        #print(matrix[i].mean())\n",
    "        temp = (matrix[i].mean())\n",
    "        arrmean.append(temp)\n",
    "    return arrmean\n",
    "\n",
    "temp = meancal(distance_matrix)\n",
    "\n",
    "def iqrfunc(nparray, multiplier = 1.5):\n",
    "    data = np.array(nparray)\n",
    "    q1 = np.percentile(data,25)\n",
    "    q3 = np.percentile(data,75)\n",
    "    iqr = q3 -q1\n",
    "    lower_bound = q1-(multiplier*1.5)\n",
    "    upper_bound = q3+(multiplier*1.5)\n",
    "    outliers = np.where((data<lower_bound) | (data>upper_bound))[0]\n",
    "    return outliers\n",
    "                        \n",
    "iqrfunc(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a0e51f",
   "metadata": {},
   "source": [
    "## Delta Score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8164f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta Score Matrix:\n",
      "[[  0. 667. 613. 516. 477. 457. 505. 504. 470. 455.]\n",
      " [667.   0. 993. 973. 969. 940. 938. 955. 901. 901.]\n",
      " [613. 993.   0. 893. 927. 928. 926. 886. 884. 903.]\n",
      " [516. 973. 893.   0. 793. 844. 886. 848. 752. 783.]\n",
      " [477. 969. 927. 793.   0. 750. 810. 808. 743. 787.]\n",
      " [457. 940. 928. 844. 750.   0. 839. 782. 754. 767.]\n",
      " [505. 938. 926. 886. 810. 839.   0. 816. 821. 792.]\n",
      " [504. 955. 886. 848. 808. 782. 816.   0. 764. 807.]\n",
      " [470. 901. 884. 752. 743. 754. 821. 764.   0. 740.]\n",
      " [455. 901. 903. 783. 787. 767. 792. 807. 740.   0.]]\n"
     ]
    }
   ],
   "source": [
    "def delta_score(model1, model2, x_test, threshold=0.1):\n",
    "    # Make predictions on the test data\n",
    "    predictions1 = model1.predict(x_test)\n",
    "    predictions2 = model2.predict(x_test)\n",
    "    \n",
    "    # Get the predicted labels and their probabilities\n",
    "    labels1 = np.argmax(predictions1, axis=1)\n",
    "    labels2 = np.argmax(predictions2, axis=1)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(labels1)):\n",
    "        if labels1[i] == labels2[i]:  # If the predicted labels are the same\n",
    "            prob1 = predictions1[i][labels1[i]]\n",
    "            prob2 = predictions2[i][labels2[i]]\n",
    "            if abs(prob1 - prob2) > threshold:  # If the absolute difference in probabilities is greater than the threshold\n",
    "                count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Initialize the distance matrix\n",
    "num_clients = len(client_models_m)\n",
    "delta_score_matrix = np.zeros((num_clients, num_clients))\n",
    "\n",
    "# Calculate the delta score for each pair of models\n",
    "for i in range(num_clients):\n",
    "    for j in range(i + 1, num_clients):\n",
    "        delta_score_matrix[i, j] = delta_score(client_models_m[i], client_models_m[j], x_test_m)\n",
    "        delta_score_matrix[j, i] = delta_score_matrix[i, j]  # Symmetric matrix\n",
    "\n",
    "# Print the delta score matrix\n",
    "print(\"Delta Score Matrix:\")\n",
    "print(delta_score_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f297e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2 = meancal(delta_score_matrix) \n",
    "iqrfunc(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccac38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9d9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c4c9d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.1070690694807714"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "sample1 = np.random.normal(0, 1, 1000)\n",
    "sample2 = np.random.normal(2, 1, 1000)\n",
    "\n",
    "# sample1.shape\n",
    "# sample1.max()\n",
    "# sample1.min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e8c88",
   "metadata": {},
   "source": [
    "## F ks begins here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd6a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be9577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i want to use ks test \n",
    "# we have 10 client and 10 models respectfully\n",
    "# for each sample (test data ) we need to find the prediction class of each model(argmax) \n",
    "# and for each sample each time find the most frequent predicted class (the one that happened more between 10 clients) and after that \n",
    "# for all models (clients) in a 2 by 2 order we are supposed to find the prediction(not argmax this time) of the most frequent class regardless of what  label the model predicted and form an array of predictions (10000 samples results in an array with the size 10k) and pass the two arrays to the ks_2samp \n",
    "# finally put the D value (not p value) in the distance matrix and after forming a matrix then utilizing the iqr test to find outliers( 0 in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a29bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS statistic: 0.676\n",
      "p-value: 1.3605932331569177e-217\n"
     ]
    }
   ],
   "source": [
    "# Perform the two-sample KS test\n",
    "D, p_value = ks_2samp(sample1, sample2)\n",
    "\n",
    "print(\"KS statistic:\", D)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d596e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_limited = x_test_m[:200]\n",
    "y_test_limited = y_test_m[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "621ebe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 10\n",
      "sample prediction 0 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 1 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 2 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 3 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 4 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 5 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 6 [9, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 7 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 8 [6, 6, 6, 5, 4, 5, 5, 6, 6, 6]\n",
      "sample prediction 9 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 10 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 11 [2, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 12 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 13 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 14 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 15 [3, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 16 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 17 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 18 [3, 3, 3, 3, 3, 3, 2, 3, 3, 3]\n",
      "sample prediction 19 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 20 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 21 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 22 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 23 [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 24 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 25 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 26 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 27 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 28 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 29 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 30 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 31 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 32 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 33 [0, 6, 6, 0, 4, 4, 6, 6, 6, 6]\n",
      "sample prediction 34 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 35 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 36 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 37 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 38 [2, 2, 3, 3, 3, 2, 2, 2, 2, 3]\n",
      "sample prediction 39 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 40 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 41 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 42 [9, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 43 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 44 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 45 [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 46 [8, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 47 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 48 [9, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 49 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 50 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 51 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 52 [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 53 [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 54 [2, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 55 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 56 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 57 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 58 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 59 [0, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 60 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 61 [8, 8, 8, 8, 8, 8, 2, 8, 8, 8]\n",
      "sample prediction 62 [4, 5, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 63 [2, 3, 3, 3, 3, 2, 3, 3, 3, 3]\n",
      "sample prediction 64 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 65 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 66 [2, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 67 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 68 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 69 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 70 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 71 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 72 [2, 2, 3, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 73 [8, 9, 9, 9, 9, 9, 9, 9, 7, 9]\n",
      "sample prediction 74 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 75 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 76 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 77 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 78 [8, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 79 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 80 [9, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 81 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 82 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 83 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 84 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 85 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 86 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 87 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 88 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 89 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 90 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 91 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 92 [8, 9, 4, 4, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 93 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 94 [8, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 95 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 96 [8, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 97 [8, 7, 3, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 98 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 99 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 100 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 101 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 102 [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 103 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 104 [4, 9, 5, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 105 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 106 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 107 [8, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 108 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 109 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 110 [8, 3, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 111 [8, 1, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 112 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 113 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 114 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 115 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 116 [9, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 117 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 118 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 119 [8, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 120 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 121 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 122 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 123 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 124 [9, 7, 4, 7, 7, 4, 7, 4, 7, 7]\n",
      "sample prediction 125 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 126 [0, 0, 0, 5, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 127 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 128 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 129 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 130 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 131 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 132 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 133 [2, 7, 2, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 134 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 135 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 136 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 137 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 138 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 139 [9, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 140 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 141 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 142 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 143 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 144 [9, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 145 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 146 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 147 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 148 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 149 [8, 9, 2, 9, 2, 4, 4, 8, 4, 9]\n",
      "sample prediction 150 [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 151 [8, 8, 8, 3, 8, 8, 9, 8, 8, 8]\n",
      "sample prediction 152 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 153 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 154 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 155 [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 156 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 157 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 158 [3, 3, 3, 3, 3, 2, 3, 3, 3, 3]\n",
      "sample prediction 159 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 160 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 161 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 162 [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 163 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 164 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 165 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 166 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 167 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 168 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 169 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 170 [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 171 [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 172 [3, 2, 3, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 173 [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 174 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 175 [8, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
      "sample prediction 176 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 177 [8, 5, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 178 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 179 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 180 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 181 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 182 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 183 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 184 [8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "sample prediction 185 [4, 4, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "sample prediction 186 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "sample prediction 187 [8, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "sample prediction 188 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 189 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 190 [8, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 191 [8, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 192 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 193 [9, 4, 3, 9, 9, 9, 9, 4, 9, 3]\n",
      "sample prediction 194 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sample prediction 195 [8, 3, 3, 8, 3, 3, 3, 3, 3, 3]\n",
      "sample prediction 196 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "sample prediction 197 [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "sample prediction 198 [9, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "sample prediction 199 [2, 2, 2, 3, 2, 2, 2, 2, 2, 2]\n",
      "how many samples? 200\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [9, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [6, 6, 6, 5, 4, 5, 5, 6, 6, 6],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [3, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [3, 3, 3, 3, 3, 3, 2, 3, 3, 3],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 6, 6, 0, 4, 4, 6, 6, 6, 6],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 2, 3, 3, 3, 2, 2, 2, 2, 3],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [9, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [9, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [2, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [0, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [8, 8, 8, 8, 8, 8, 2, 8, 8, 8],\n",
       " [4, 5, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [2, 3, 3, 3, 3, 2, 3, 3, 3, 3],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [2, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [2, 2, 3, 2, 2, 2, 2, 2, 2, 2],\n",
       " [8, 9, 9, 9, 9, 9, 9, 9, 7, 9],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [8, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [9, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [8, 9, 4, 4, 9, 9, 9, 9, 9, 9],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [8, 7, 3, 7, 7, 7, 7, 7, 7, 7],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [4, 9, 5, 9, 9, 9, 9, 9, 9, 9],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [8, 3, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [8, 1, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [9, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [8, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [9, 7, 4, 7, 7, 4, 7, 4, 7, 7],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [0, 0, 0, 5, 0, 0, 0, 0, 0, 0],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [2, 7, 2, 7, 7, 7, 7, 7, 7, 7],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [9, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [9, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [8, 9, 2, 9, 2, 4, 4, 8, 4, 9],\n",
       " [9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [8, 8, 8, 3, 8, 8, 9, 8, 8, 8],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [3, 3, 3, 3, 3, 2, 3, 3, 3, 3],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [3, 2, 3, 2, 2, 2, 2, 2, 2, 2],\n",
       " [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [8, 7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [8, 5, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       " [4, 4, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       " [8, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [8, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [9, 4, 3, 9, 9, 9, 9, 4, 9, 3],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [8, 3, 3, 8, 3, 3, 3, 3, 3, 3],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       " [9, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
       " [2, 2, 2, 3, 2, 2, 2, 2, 2, 2]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ks_test_for_client_models(client_models, x_test):\n",
    "\n",
    "    all_sample_predictions = []\n",
    "\n",
    "    print(f\"Number of clients: {len(client_models)}\")\n",
    "\n",
    "    sample_index = 0  # Initialize a sample index\n",
    "\n",
    "    for temp_sample in x_test:\n",
    "\n",
    "        sample_predictions = []\n",
    "\n",
    "        for temp_client_model in client_models:\n",
    "            predicted_class = temp_client_model.predict(np.expand_dims(temp_sample, axis=0)).argmax(axis=1)[0]\n",
    "            sample_predictions.append(predicted_class)\n",
    "\n",
    "        all_sample_predictions.append(sample_predictions)\n",
    "        print(f\"sample prediction {sample_index}\", sample_predictions)  # Use sample_index\n",
    "\n",
    "        sample_index += 1  # Increment the sample index\n",
    "\n",
    "    return all_sample_predictions\n",
    "\n",
    "\n",
    "sample_predictions_all = ks_test_for_client_models(client_models_m, x_test_limited)\n",
    "print(\"*\"*40)\n",
    "print(\"how many samples?\",len(sample_predictions_all))\n",
    "print(\"*\"*40)\n",
    "sample_predictions_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f574293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 6, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4, 1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2, 5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1, 7, 1, 8, 2, 0, 9, 9, 8, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5, 1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1, 0, 9, 0, 3, 1, 6, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent_classes(predictions_all):\n",
    "    most_frequent_classes_list = []\n",
    "    \n",
    "    # Iterate over the predictions for each sample\n",
    "    for sample_predictions in predictions_all:\n",
    "        # Count the frequency of each predicted class\n",
    "        class_counter = Counter(sample_predictions)\n",
    "        # Find the class with the highest count (the most frequent)\n",
    "        most_frequent_class = class_counter.most_common(1)[0][0]\n",
    "        # Append the most frequent class to the list\n",
    "        most_frequent_classes_list.append(most_frequent_class)\n",
    "    \n",
    "    return most_frequent_classes_list\n",
    "\n",
    "# Find the most frequent classes for all test samples\n",
    "most_frequent_classes_list = most_frequent_classes(sample_predictions_all)\n",
    "print(most_frequent_classes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "555cd088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1.0,\n",
       "  0.9997439,\n",
       "  0.9978787,\n",
       "  0.9999795,\n",
       "  0.99999416,\n",
       "  0.9999944,\n",
       "  0.9985039,\n",
       "  0.9999939,\n",
       "  0.9999939,\n",
       "  0.99996316],\n",
       " [1.0,\n",
       "  0.999356,\n",
       "  0.9980634,\n",
       "  0.99838805,\n",
       "  0.99980754,\n",
       "  0.99999833,\n",
       "  0.9999989,\n",
       "  0.99993,\n",
       "  0.999276,\n",
       "  0.9975274],\n",
       " [1.0,\n",
       "  0.9999875,\n",
       "  0.99989367,\n",
       "  0.9999155,\n",
       "  0.999897,\n",
       "  0.9999982,\n",
       "  0.9995283,\n",
       "  0.9985997,\n",
       "  0.9994361,\n",
       "  0.99940836],\n",
       " [1.0,\n",
       "  0.99956304,\n",
       "  0.9998037,\n",
       "  0.9999263,\n",
       "  0.9999976,\n",
       "  0.9999832,\n",
       "  0.99978644,\n",
       "  0.99991846,\n",
       "  0.99975055,\n",
       "  0.99997425],\n",
       " [1.0,\n",
       "  0.9913312,\n",
       "  0.9949831,\n",
       "  0.97548467,\n",
       "  0.9976047,\n",
       "  0.99943477,\n",
       "  0.9600428,\n",
       "  0.9974331,\n",
       "  0.99996626,\n",
       "  0.9958587],\n",
       " [1.0,\n",
       "  0.9999889,\n",
       "  0.99997926,\n",
       "  0.99998415,\n",
       "  0.9998878,\n",
       "  0.99999964,\n",
       "  0.9999454,\n",
       "  0.9996456,\n",
       "  0.999731,\n",
       "  0.99994314],\n",
       " [0.07265521,\n",
       "  0.9998171,\n",
       "  0.99989223,\n",
       "  0.998295,\n",
       "  0.99978524,\n",
       "  0.99923694,\n",
       "  0.998747,\n",
       "  0.9998136,\n",
       "  0.99509287,\n",
       "  0.97221947],\n",
       " [1.0,\n",
       "  0.88819695,\n",
       "  0.99644285,\n",
       "  0.9980034,\n",
       "  0.9943493,\n",
       "  0.96358323,\n",
       "  0.99782455,\n",
       "  0.982882,\n",
       "  0.9347654,\n",
       "  0.9976489],\n",
       " [1.0,\n",
       "  0.9517057,\n",
       "  0.5933358,\n",
       "  0.11509766,\n",
       "  0.106663875,\n",
       "  0.45051548,\n",
       "  0.05841753,\n",
       "  0.57605094,\n",
       "  0.80668557,\n",
       "  0.7627288],\n",
       " [1.0,\n",
       "  0.99345183,\n",
       "  0.98681355,\n",
       "  0.99846905,\n",
       "  0.9973405,\n",
       "  0.9729897,\n",
       "  0.9902665,\n",
       "  0.98906404,\n",
       "  0.8214666,\n",
       "  0.9978072],\n",
       " [1.0,\n",
       "  0.99998987,\n",
       "  0.9998822,\n",
       "  0.99999046,\n",
       "  0.9999995,\n",
       "  0.99999785,\n",
       "  0.9999529,\n",
       "  0.99991834,\n",
       "  0.9999634,\n",
       "  0.9999856],\n",
       " [0.0,\n",
       "  0.9987068,\n",
       "  0.9905201,\n",
       "  0.98849094,\n",
       "  0.98808575,\n",
       "  0.97808665,\n",
       "  0.999424,\n",
       "  0.982496,\n",
       "  0.99858224,\n",
       "  0.9956132],\n",
       " [1.0,\n",
       "  0.974358,\n",
       "  0.99762195,\n",
       "  0.99993694,\n",
       "  0.9993123,\n",
       "  0.99919325,\n",
       "  0.99798524,\n",
       "  0.9976488,\n",
       "  0.999803,\n",
       "  0.99951696],\n",
       " [1.0,\n",
       "  0.9908119,\n",
       "  0.9996443,\n",
       "  0.9999839,\n",
       "  0.9999995,\n",
       "  0.9999958,\n",
       "  0.99978346,\n",
       "  0.9999851,\n",
       "  0.9999709,\n",
       "  0.99999666],\n",
       " [1.0,\n",
       "  0.9999994,\n",
       "  0.9999919,\n",
       "  0.99999094,\n",
       "  0.9999888,\n",
       "  0.9999994,\n",
       "  0.9999815,\n",
       "  0.99995136,\n",
       "  0.9999881,\n",
       "  0.9997875],\n",
       " [0.0,\n",
       "  0.97781575,\n",
       "  0.99143374,\n",
       "  0.9996735,\n",
       "  0.99711585,\n",
       "  0.9686581,\n",
       "  0.99067736,\n",
       "  0.98095345,\n",
       "  0.99094486,\n",
       "  0.9843205],\n",
       " [1.0,\n",
       "  0.99743134,\n",
       "  0.9964055,\n",
       "  0.99961495,\n",
       "  0.9975916,\n",
       "  0.9971629,\n",
       "  0.996396,\n",
       "  0.9876632,\n",
       "  0.9993537,\n",
       "  0.999166],\n",
       " [1.0,\n",
       "  0.994436,\n",
       "  0.88755566,\n",
       "  0.99998903,\n",
       "  0.9999913,\n",
       "  0.99998415,\n",
       "  0.99590933,\n",
       "  0.9999956,\n",
       "  0.9999932,\n",
       "  0.99997663],\n",
       " [1.0,\n",
       "  0.9004667,\n",
       "  0.9757608,\n",
       "  0.9913378,\n",
       "  0.96114254,\n",
       "  0.47235596,\n",
       "  0.11671012,\n",
       "  0.55627716,\n",
       "  0.992859,\n",
       "  0.9499891],\n",
       " [1.0,\n",
       "  0.99976605,\n",
       "  0.9999863,\n",
       "  0.9996309,\n",
       "  0.9999943,\n",
       "  0.9998939,\n",
       "  0.99980384,\n",
       "  0.9999759,\n",
       "  0.9997038,\n",
       "  0.9997681],\n",
       " [1.0,\n",
       "  0.9850418,\n",
       "  0.98841906,\n",
       "  0.9997788,\n",
       "  0.9963379,\n",
       "  0.92091924,\n",
       "  0.9989839,\n",
       "  0.99951375,\n",
       "  0.80596703,\n",
       "  0.99650085],\n",
       " [1.0,\n",
       "  0.9997242,\n",
       "  0.99991155,\n",
       "  0.9880855,\n",
       "  0.99836653,\n",
       "  0.99963605,\n",
       "  0.9955421,\n",
       "  0.9993845,\n",
       "  0.9981186,\n",
       "  0.9961134],\n",
       " [1.0,\n",
       "  0.9998241,\n",
       "  0.99729055,\n",
       "  0.90821826,\n",
       "  0.9993863,\n",
       "  0.99434376,\n",
       "  0.99983764,\n",
       "  0.9014217,\n",
       "  0.9992737,\n",
       "  0.99972504],\n",
       " [1.0,\n",
       "  0.99999905,\n",
       "  0.99999714,\n",
       "  0.999979,\n",
       "  0.99988496,\n",
       "  0.99999607,\n",
       "  0.9999982,\n",
       "  0.9999938,\n",
       "  0.9999807,\n",
       "  0.999977],\n",
       " [1.0,\n",
       "  0.99740285,\n",
       "  0.99936336,\n",
       "  0.9787154,\n",
       "  0.99696845,\n",
       "  0.9968516,\n",
       "  0.97739464,\n",
       "  0.99055696,\n",
       "  0.99958724,\n",
       "  0.9975585],\n",
       " [1.0,\n",
       "  0.9999987,\n",
       "  0.99999905,\n",
       "  0.9999981,\n",
       "  0.9999995,\n",
       "  1.0,\n",
       "  0.99997735,\n",
       "  0.9999995,\n",
       "  0.99998236,\n",
       "  0.99999964],\n",
       " [1.0,\n",
       "  0.9993119,\n",
       "  0.99981946,\n",
       "  0.9998541,\n",
       "  0.99992204,\n",
       "  0.9333999,\n",
       "  0.95135784,\n",
       "  0.99968636,\n",
       "  0.9996519,\n",
       "  0.9966922],\n",
       " [1.0,\n",
       "  0.9994848,\n",
       "  0.99995995,\n",
       "  0.99923563,\n",
       "  0.9999584,\n",
       "  0.999463,\n",
       "  0.9998493,\n",
       "  0.9999846,\n",
       "  0.99993443,\n",
       "  0.99873513],\n",
       " [1.0,\n",
       "  0.9999559,\n",
       "  0.99969923,\n",
       "  0.99999833,\n",
       "  0.99999964,\n",
       "  0.99973994,\n",
       "  0.9999473,\n",
       "  0.99997747,\n",
       "  0.99995923,\n",
       "  0.99999785],\n",
       " [1.0,\n",
       "  0.99986446,\n",
       "  0.9999856,\n",
       "  0.99904317,\n",
       "  0.9996247,\n",
       "  0.999936,\n",
       "  0.99987173,\n",
       "  0.9998605,\n",
       "  0.9996823,\n",
       "  0.9998018],\n",
       " [1.0,\n",
       "  0.9998684,\n",
       "  0.99997675,\n",
       "  0.99969804,\n",
       "  0.9994795,\n",
       "  0.99984705,\n",
       "  0.99958724,\n",
       "  0.99895716,\n",
       "  0.9960872,\n",
       "  0.9999089],\n",
       " [1.0,\n",
       "  0.9994234,\n",
       "  0.99974877,\n",
       "  0.98560053,\n",
       "  0.99384815,\n",
       "  0.99856406,\n",
       "  0.99949884,\n",
       "  0.99839574,\n",
       "  0.99818474,\n",
       "  0.9944131],\n",
       " [1.0,\n",
       "  0.9996902,\n",
       "  0.99999297,\n",
       "  0.9999844,\n",
       "  0.9988557,\n",
       "  0.99997556,\n",
       "  0.9999815,\n",
       "  0.99999344,\n",
       "  0.99949956,\n",
       "  0.99998486],\n",
       " [0.0,\n",
       "  0.9761455,\n",
       "  0.66879267,\n",
       "  0.050316077,\n",
       "  0.0003819651,\n",
       "  0.11441541,\n",
       "  0.64253664,\n",
       "  0.8184163,\n",
       "  0.63747454,\n",
       "  0.91413116],\n",
       " [1.0,\n",
       "  0.99876344,\n",
       "  0.94614846,\n",
       "  0.9994892,\n",
       "  0.99999046,\n",
       "  0.99999046,\n",
       "  0.9986811,\n",
       "  0.9998124,\n",
       "  0.99995685,\n",
       "  0.99998343],\n",
       " [1.0,\n",
       "  0.9997465,\n",
       "  0.9645862,\n",
       "  0.99926883,\n",
       "  0.9998814,\n",
       "  0.9999995,\n",
       "  0.99999654,\n",
       "  0.99999475,\n",
       "  0.9999558,\n",
       "  0.99919134],\n",
       " [1.0,\n",
       "  0.9897279,\n",
       "  0.6212769,\n",
       "  0.99993634,\n",
       "  0.9997956,\n",
       "  0.9925949,\n",
       "  0.9960155,\n",
       "  0.98044205,\n",
       "  0.9931312,\n",
       "  0.99690384],\n",
       " [1.0,\n",
       "  0.9999974,\n",
       "  0.9999995,\n",
       "  0.99962974,\n",
       "  0.99985373,\n",
       "  0.99999416,\n",
       "  0.99997795,\n",
       "  0.9995628,\n",
       "  0.9999535,\n",
       "  0.9999448],\n",
       " [1.0,\n",
       "  0.65763855,\n",
       "  0.066486195,\n",
       "  0.36694887,\n",
       "  0.44772387,\n",
       "  0.9350124,\n",
       "  0.76915365,\n",
       "  0.8526949,\n",
       "  0.6364685,\n",
       "  0.37392908],\n",
       " [1.0,\n",
       "  0.9999993,\n",
       "  0.99995506,\n",
       "  0.99996674,\n",
       "  0.99961,\n",
       "  0.9998994,\n",
       "  0.99922395,\n",
       "  0.99886215,\n",
       "  0.9999541,\n",
       "  0.9998254],\n",
       " [1.0,\n",
       "  0.9994879,\n",
       "  0.9999484,\n",
       "  0.99848866,\n",
       "  0.99811375,\n",
       "  0.99982065,\n",
       "  0.99926,\n",
       "  0.9974101,\n",
       "  0.99915075,\n",
       "  0.99812967],\n",
       " [1.0,\n",
       "  0.8777792,\n",
       "  0.99192554,\n",
       "  0.99810106,\n",
       "  0.9981184,\n",
       "  0.994622,\n",
       "  0.9834335,\n",
       "  0.99345124,\n",
       "  0.9984907,\n",
       "  0.997931],\n",
       " [0.0,\n",
       "  0.9996611,\n",
       "  0.99999845,\n",
       "  0.9978855,\n",
       "  0.999915,\n",
       "  0.999759,\n",
       "  0.9974732,\n",
       "  0.99994326,\n",
       "  0.9996458,\n",
       "  0.9949568],\n",
       " [1.0,\n",
       "  0.9903357,\n",
       "  0.9935923,\n",
       "  0.98909515,\n",
       "  0.9898863,\n",
       "  0.9986904,\n",
       "  0.9985821,\n",
       "  0.99725705,\n",
       "  0.9980508,\n",
       "  0.9986234],\n",
       " [1.0,\n",
       "  0.9991246,\n",
       "  0.99976104,\n",
       "  0.9963031,\n",
       "  0.95639354,\n",
       "  0.9916453,\n",
       "  0.96895707,\n",
       "  0.99710244,\n",
       "  0.9069415,\n",
       "  0.9988753],\n",
       " [1.0,\n",
       "  0.99956244,\n",
       "  0.9898896,\n",
       "  0.9888314,\n",
       "  0.9931443,\n",
       "  0.9965146,\n",
       "  0.99943274,\n",
       "  0.99946886,\n",
       "  0.9976654,\n",
       "  0.9767864],\n",
       " [0.0,\n",
       "  0.88126683,\n",
       "  0.9976857,\n",
       "  0.71797997,\n",
       "  0.9905894,\n",
       "  0.99802774,\n",
       "  0.9913796,\n",
       "  0.99110585,\n",
       "  0.9217565,\n",
       "  0.9570959],\n",
       " [1.0,\n",
       "  0.9998282,\n",
       "  0.9997367,\n",
       "  0.99327236,\n",
       "  0.9997421,\n",
       "  0.99989057,\n",
       "  0.9986808,\n",
       "  0.9996939,\n",
       "  0.9979948,\n",
       "  0.9996099],\n",
       " [0.0028621026,\n",
       "  0.9999019,\n",
       "  0.9999521,\n",
       "  0.9999238,\n",
       "  0.9999734,\n",
       "  0.99984765,\n",
       "  0.9999026,\n",
       "  0.9999881,\n",
       "  0.99995756,\n",
       "  0.99852717],\n",
       " [1.0,\n",
       "  0.9998771,\n",
       "  0.9999815,\n",
       "  0.9994741,\n",
       "  0.9998952,\n",
       "  0.99957484,\n",
       "  0.9993505,\n",
       "  0.9999714,\n",
       "  0.9999982,\n",
       "  0.99897647],\n",
       " [1.0,\n",
       "  0.9998661,\n",
       "  0.9997706,\n",
       "  0.9992487,\n",
       "  0.99729997,\n",
       "  0.99879336,\n",
       "  0.9955373,\n",
       "  0.9999372,\n",
       "  0.9979602,\n",
       "  0.9963575],\n",
       " [1.0,\n",
       "  0.99990225,\n",
       "  0.9998067,\n",
       "  0.9999485,\n",
       "  0.99942774,\n",
       "  0.9979699,\n",
       "  0.99624676,\n",
       "  0.99950564,\n",
       "  0.99968874,\n",
       "  0.9848567],\n",
       " [1.0,\n",
       "  0.99983716,\n",
       "  0.9995566,\n",
       "  0.99815506,\n",
       "  0.99610716,\n",
       "  0.9977437,\n",
       "  0.99990654,\n",
       "  0.9994906,\n",
       "  0.999551,\n",
       "  0.9981365],\n",
       " [1.0,\n",
       "  0.9928294,\n",
       "  0.99184847,\n",
       "  0.99588543,\n",
       "  0.98070395,\n",
       "  0.8373325,\n",
       "  0.99868745,\n",
       "  0.9899442,\n",
       "  0.98978066,\n",
       "  0.9094658],\n",
       " [0.0,\n",
       "  0.99999917,\n",
       "  0.9999577,\n",
       "  0.99955434,\n",
       "  0.99991155,\n",
       "  0.9983461,\n",
       "  0.9972812,\n",
       "  0.9965612,\n",
       "  0.9999465,\n",
       "  0.9974758],\n",
       " [1.0,\n",
       "  0.90638417,\n",
       "  0.99180275,\n",
       "  0.99896085,\n",
       "  0.9999397,\n",
       "  0.997898,\n",
       "  0.99556535,\n",
       "  0.96278524,\n",
       "  0.9989833,\n",
       "  0.99986196],\n",
       " [1.0,\n",
       "  0.9999958,\n",
       "  0.9999995,\n",
       "  0.9999993,\n",
       "  0.99996066,\n",
       "  0.99999595,\n",
       "  0.99997723,\n",
       "  0.9999975,\n",
       "  0.9999994,\n",
       "  0.9998909],\n",
       " [1.0,\n",
       "  0.99998665,\n",
       "  0.99988914,\n",
       "  0.9999182,\n",
       "  0.9996118,\n",
       "  0.99999666,\n",
       "  0.9998567,\n",
       "  0.9978963,\n",
       "  0.99920744,\n",
       "  0.9989454],\n",
       " [1.0,\n",
       "  0.9974988,\n",
       "  0.99696034,\n",
       "  0.99986696,\n",
       "  0.99962544,\n",
       "  0.9996865,\n",
       "  0.9922775,\n",
       "  0.9887714,\n",
       "  0.99907315,\n",
       "  0.99984],\n",
       " [5.416144e-07,\n",
       "  0.9917584,\n",
       "  0.9219381,\n",
       "  0.9022439,\n",
       "  0.9959287,\n",
       "  0.9227677,\n",
       "  0.9969015,\n",
       "  0.9770429,\n",
       "  0.93235284,\n",
       "  0.979124],\n",
       " [1.0,\n",
       "  0.9996469,\n",
       "  0.9999136,\n",
       "  0.9999999,\n",
       "  0.99997556,\n",
       "  0.999966,\n",
       "  0.97946036,\n",
       "  0.9999051,\n",
       "  0.99999726,\n",
       "  0.9999907],\n",
       " [1.0,\n",
       "  0.9968273,\n",
       "  0.99351764,\n",
       "  0.99453485,\n",
       "  0.98345804,\n",
       "  0.65035903,\n",
       "  0.33351752,\n",
       "  0.97773075,\n",
       "  0.997274,\n",
       "  0.94080025],\n",
       " [2.4808661e-21,\n",
       "  0.22086851,\n",
       "  0.5566263,\n",
       "  0.98343015,\n",
       "  0.7998391,\n",
       "  0.8959662,\n",
       "  0.83638704,\n",
       "  0.8252968,\n",
       "  0.92908275,\n",
       "  0.6087216],\n",
       " [0.0,\n",
       "  0.9486718,\n",
       "  0.9994797,\n",
       "  0.9948949,\n",
       "  0.86618596,\n",
       "  0.13781954,\n",
       "  0.9480214,\n",
       "  0.74190086,\n",
       "  0.9759092,\n",
       "  0.99254346],\n",
       " [1.0,\n",
       "  0.95258754,\n",
       "  0.6904148,\n",
       "  0.9955841,\n",
       "  0.9991954,\n",
       "  0.9999739,\n",
       "  0.99552935,\n",
       "  0.99995935,\n",
       "  0.9990004,\n",
       "  0.9999908],\n",
       " [1.0,\n",
       "  0.9636023,\n",
       "  0.9661053,\n",
       "  0.9879054,\n",
       "  0.736003,\n",
       "  0.94580036,\n",
       "  0.96310765,\n",
       "  0.9851718,\n",
       "  0.6673914,\n",
       "  0.9975262],\n",
       " [0.0,\n",
       "  0.9931293,\n",
       "  0.7896861,\n",
       "  0.86817384,\n",
       "  0.8295419,\n",
       "  0.83732915,\n",
       "  0.9901668,\n",
       "  0.55585605,\n",
       "  0.96126455,\n",
       "  0.96351695],\n",
       " [1.0,\n",
       "  0.99989617,\n",
       "  0.99995434,\n",
       "  0.99999905,\n",
       "  0.99991846,\n",
       "  0.99997854,\n",
       "  0.99995804,\n",
       "  0.9999602,\n",
       "  0.99999,\n",
       "  0.99963033],\n",
       " [1.0,\n",
       "  0.99997234,\n",
       "  0.9999995,\n",
       "  0.99998224,\n",
       "  0.9997819,\n",
       "  0.9999548,\n",
       "  0.9997392,\n",
       "  0.99841654,\n",
       "  0.9993486,\n",
       "  0.99993825],\n",
       " [1.0,\n",
       "  0.9999981,\n",
       "  0.9999137,\n",
       "  0.99996924,\n",
       "  0.9999316,\n",
       "  0.9999994,\n",
       "  0.99999666,\n",
       "  0.9999974,\n",
       "  0.9999716,\n",
       "  0.99998295],\n",
       " [1.0,\n",
       "  0.9992574,\n",
       "  0.9992693,\n",
       "  0.9999974,\n",
       "  0.99999166,\n",
       "  0.99999905,\n",
       "  0.9997347,\n",
       "  1.0,\n",
       "  0.99998987,\n",
       "  0.9999968],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  0.9999995,\n",
       "  0.99999976,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.99999976,\n",
       "  0.9999999,\n",
       "  0.9999994,\n",
       "  0.9999999],\n",
       " [1.0,\n",
       "  0.9996438,\n",
       "  0.27996165,\n",
       "  0.9165767,\n",
       "  0.9671027,\n",
       "  0.9999844,\n",
       "  0.9998703,\n",
       "  0.99984217,\n",
       "  0.99991834,\n",
       "  0.8248143],\n",
       " [0.0,\n",
       "  0.31485987,\n",
       "  0.9885175,\n",
       "  0.99916136,\n",
       "  0.8712486,\n",
       "  0.9075128,\n",
       "  0.98957986,\n",
       "  0.97755754,\n",
       "  0.445962,\n",
       "  0.92351186],\n",
       " [1.0,\n",
       "  0.9999976,\n",
       "  0.99999607,\n",
       "  0.99930847,\n",
       "  0.9997428,\n",
       "  0.99999475,\n",
       "  0.9999286,\n",
       "  0.9992919,\n",
       "  0.9998834,\n",
       "  0.9999286],\n",
       " [1.0,\n",
       "  0.9992083,\n",
       "  0.99386615,\n",
       "  0.99905723,\n",
       "  0.9991928,\n",
       "  0.9978053,\n",
       "  0.9997558,\n",
       "  0.9997571,\n",
       "  0.99980634,\n",
       "  0.9999068],\n",
       " [1.0,\n",
       "  0.9956898,\n",
       "  0.9999784,\n",
       "  0.99969566,\n",
       "  0.9964645,\n",
       "  0.99976736,\n",
       "  0.99963117,\n",
       "  0.999652,\n",
       "  0.9989818,\n",
       "  0.9998568],\n",
       " [1.0,\n",
       "  0.9972012,\n",
       "  0.978366,\n",
       "  0.95416737,\n",
       "  0.7816535,\n",
       "  0.9847704,\n",
       "  0.99952686,\n",
       "  0.99780375,\n",
       "  0.9629325,\n",
       "  0.72262156],\n",
       " [0.0,\n",
       "  0.99159765,\n",
       "  0.93430835,\n",
       "  0.98654556,\n",
       "  0.9921523,\n",
       "  0.98544264,\n",
       "  0.9989994,\n",
       "  0.9476928,\n",
       "  0.9899503,\n",
       "  0.98369664],\n",
       " [1.0,\n",
       "  0.99855095,\n",
       "  0.9993799,\n",
       "  0.99997175,\n",
       "  0.9997937,\n",
       "  0.9999939,\n",
       "  0.999877,\n",
       "  0.999997,\n",
       "  0.99999845,\n",
       "  0.9999994],\n",
       " [0.0,\n",
       "  0.9038259,\n",
       "  0.9865537,\n",
       "  0.898423,\n",
       "  0.818733,\n",
       "  0.7232009,\n",
       "  0.663742,\n",
       "  0.96746963,\n",
       "  0.9832012,\n",
       "  0.88902473],\n",
       " [1.0,\n",
       "  0.99816906,\n",
       "  0.9986387,\n",
       "  0.9851639,\n",
       "  0.9592323,\n",
       "  0.99864036,\n",
       "  0.997865,\n",
       "  0.9994186,\n",
       "  0.9490172,\n",
       "  0.9957957],\n",
       " [1.0,\n",
       "  0.9999999,\n",
       "  0.99994814,\n",
       "  0.99985063,\n",
       "  0.99999917,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999999,\n",
       "  1.0,\n",
       "  0.9999949],\n",
       " [1.0,\n",
       "  0.99968874,\n",
       "  0.99967766,\n",
       "  0.9966184,\n",
       "  0.988801,\n",
       "  0.997649,\n",
       "  0.98347247,\n",
       "  0.9992619,\n",
       "  0.9998041,\n",
       "  0.99893874],\n",
       " [1.0,\n",
       "  0.90379125,\n",
       "  0.9974261,\n",
       "  0.99991167,\n",
       "  0.97068787,\n",
       "  0.9989139,\n",
       "  0.8832655,\n",
       "  0.96503377,\n",
       "  0.9837895,\n",
       "  0.9888749],\n",
       " [1.0,\n",
       "  0.99999845,\n",
       "  0.9999999,\n",
       "  1.0,\n",
       "  0.9999964,\n",
       "  0.999998,\n",
       "  0.9999969,\n",
       "  0.99999976,\n",
       "  0.99999976,\n",
       "  0.9995254],\n",
       " [1.0,\n",
       "  0.9998197,\n",
       "  0.99968266,\n",
       "  0.9999459,\n",
       "  0.9999733,\n",
       "  0.9999938,\n",
       "  0.9999279,\n",
       "  0.99999845,\n",
       "  0.99998546,\n",
       "  0.9999604],\n",
       " [1.0,\n",
       "  0.93220043,\n",
       "  0.9988581,\n",
       "  0.998697,\n",
       "  0.9919658,\n",
       "  0.9915019,\n",
       "  0.9991714,\n",
       "  0.99940264,\n",
       "  0.96355385,\n",
       "  0.9993445],\n",
       " [1.0,\n",
       "  0.99999964,\n",
       "  0.9999993,\n",
       "  0.9999981,\n",
       "  0.9999982,\n",
       "  0.9999968,\n",
       "  0.99999976,\n",
       "  0.99999976,\n",
       "  0.9999964,\n",
       "  0.99999964],\n",
       " [1.0,\n",
       "  0.9999982,\n",
       "  0.99997866,\n",
       "  0.99999535,\n",
       "  0.999948,\n",
       "  0.9999993,\n",
       "  0.99977475,\n",
       "  0.9998747,\n",
       "  0.99979824,\n",
       "  0.9999708],\n",
       " [1.0,\n",
       "  0.99967015,\n",
       "  0.9999627,\n",
       "  0.9998555,\n",
       "  0.99897075,\n",
       "  0.99685436,\n",
       "  0.99968815,\n",
       "  0.99616927,\n",
       "  0.9990102,\n",
       "  0.9999901],\n",
       " [1.0,\n",
       "  0.9999988,\n",
       "  0.99999964,\n",
       "  0.99998903,\n",
       "  0.99999094,\n",
       "  0.999997,\n",
       "  0.9999832,\n",
       "  0.9999974,\n",
       "  0.99998784,\n",
       "  0.9999789],\n",
       " [0.0,\n",
       "  0.6637488,\n",
       "  0.15015276,\n",
       "  0.45530343,\n",
       "  0.97249955,\n",
       "  0.83388305,\n",
       "  0.88755304,\n",
       "  0.49350023,\n",
       "  0.70809275,\n",
       "  0.7174579],\n",
       " [1.0,\n",
       "  0.9995455,\n",
       "  0.9998797,\n",
       "  0.9975309,\n",
       "  0.9995664,\n",
       "  0.99807286,\n",
       "  0.99903035,\n",
       "  0.96354085,\n",
       "  0.98897713,\n",
       "  0.99991477],\n",
       " [2.3580701e-26,\n",
       "  0.9998061,\n",
       "  0.9998259,\n",
       "  0.99839824,\n",
       "  0.9988034,\n",
       "  0.99993503,\n",
       "  0.999923,\n",
       "  0.9998479,\n",
       "  0.9996532,\n",
       "  0.999796],\n",
       " [1.0,\n",
       "  0.9996278,\n",
       "  0.9999664,\n",
       "  0.9973086,\n",
       "  0.9997209,\n",
       "  0.9999714,\n",
       "  0.9999826,\n",
       "  0.99977237,\n",
       "  0.9998801,\n",
       "  0.997592],\n",
       " [0.0,\n",
       "  0.9852881,\n",
       "  0.9932805,\n",
       "  0.5484288,\n",
       "  0.8037407,\n",
       "  0.9852905,\n",
       "  0.99374205,\n",
       "  0.98688346,\n",
       "  0.93422717,\n",
       "  0.9457914],\n",
       " [0.0,\n",
       "  0.9824807,\n",
       "  0.25036463,\n",
       "  0.9672733,\n",
       "  0.96768904,\n",
       "  0.9971138,\n",
       "  0.9117618,\n",
       "  0.9982469,\n",
       "  0.9950511,\n",
       "  0.96664965],\n",
       " [1.0,\n",
       "  0.99410963,\n",
       "  0.9990664,\n",
       "  0.9928693,\n",
       "  0.5907444,\n",
       "  0.9995435,\n",
       "  0.98576933,\n",
       "  0.99894875,\n",
       "  0.99921834,\n",
       "  0.9925107],\n",
       " [1.0,\n",
       "  0.99989784,\n",
       "  0.9998919,\n",
       "  0.9999846,\n",
       "  0.9999708,\n",
       "  0.99997354,\n",
       "  0.99990106,\n",
       "  0.99991107,\n",
       "  0.99996173,\n",
       "  0.9998159],\n",
       " [1.0,\n",
       "  0.99992216,\n",
       "  0.9989278,\n",
       "  0.9998056,\n",
       "  0.9996625,\n",
       "  0.9985197,\n",
       "  0.99981064,\n",
       "  0.99977726,\n",
       "  0.99925345,\n",
       "  0.99941385],\n",
       " [1.0,\n",
       "  0.99989974,\n",
       "  0.9950948,\n",
       "  0.999936,\n",
       "  0.9999988,\n",
       "  0.9999981,\n",
       "  0.9999882,\n",
       "  0.999881,\n",
       "  0.9999927,\n",
       "  0.99999976],\n",
       " [1.0,\n",
       "  0.99999964,\n",
       "  0.9999999,\n",
       "  0.999998,\n",
       "  0.9999646,\n",
       "  0.9961034,\n",
       "  0.99999857,\n",
       "  0.99997735,\n",
       "  0.9999808,\n",
       "  0.9993773],\n",
       " [1.0,\n",
       "  0.999997,\n",
       "  0.9999192,\n",
       "  0.9999999,\n",
       "  0.99999785,\n",
       "  0.9999982,\n",
       "  0.99991894,\n",
       "  0.9999962,\n",
       "  1.0,\n",
       "  0.9999356],\n",
       " [4.469559e-11,\n",
       "  0.84932363,\n",
       "  0.1944216,\n",
       "  0.97615254,\n",
       "  0.837483,\n",
       "  0.9888575,\n",
       "  0.963103,\n",
       "  0.99324155,\n",
       "  0.93087673,\n",
       "  0.9947429],\n",
       " [1.0,\n",
       "  0.9992506,\n",
       "  0.99981207,\n",
       "  0.99985015,\n",
       "  0.9999019,\n",
       "  0.99977726,\n",
       "  0.9989279,\n",
       "  0.99513596,\n",
       "  0.99953115,\n",
       "  0.99905986],\n",
       " [1.0,\n",
       "  0.99985003,\n",
       "  0.8890335,\n",
       "  0.7103531,\n",
       "  0.90290695,\n",
       "  0.99996734,\n",
       "  0.9999876,\n",
       "  0.99980015,\n",
       "  0.99959654,\n",
       "  0.9943857],\n",
       " [0.0,\n",
       "  0.99962485,\n",
       "  0.99475515,\n",
       "  0.9999522,\n",
       "  0.9977108,\n",
       "  0.9998702,\n",
       "  0.9972078,\n",
       "  0.99432385,\n",
       "  0.9990089,\n",
       "  0.9997429],\n",
       " [1.0,\n",
       "  0.9787362,\n",
       "  0.9993519,\n",
       "  0.99978787,\n",
       "  0.9981791,\n",
       "  0.99571157,\n",
       "  0.99965143,\n",
       "  0.99798167,\n",
       "  0.9995939,\n",
       "  0.99230087],\n",
       " [1.0,\n",
       "  0.9930888,\n",
       "  0.99966514,\n",
       "  0.99666363,\n",
       "  0.9995881,\n",
       "  0.9979158,\n",
       "  0.9988883,\n",
       "  0.9996942,\n",
       "  0.99878126,\n",
       "  0.9862698],\n",
       " [1.0,\n",
       "  0.3678667,\n",
       "  0.9871344,\n",
       "  0.9972475,\n",
       "  0.9998437,\n",
       "  0.9999454,\n",
       "  0.98937166,\n",
       "  0.99889404,\n",
       "  0.99690586,\n",
       "  0.9962083],\n",
       " [0.0,\n",
       "  0.32147506,\n",
       "  0.769709,\n",
       "  0.8967571,\n",
       "  0.9884078,\n",
       "  0.9077125,\n",
       "  0.7495115,\n",
       "  0.99872357,\n",
       "  0.9863409,\n",
       "  0.9218918],\n",
       " [1.0,\n",
       "  0.9999429,\n",
       "  0.9999045,\n",
       "  0.99952126,\n",
       "  0.99281514,\n",
       "  0.9998085,\n",
       "  0.9913556,\n",
       "  0.9931619,\n",
       "  0.9986957,\n",
       "  0.9985862],\n",
       " [1.0,\n",
       "  0.9982204,\n",
       "  0.9999486,\n",
       "  0.9999722,\n",
       "  0.9999716,\n",
       "  0.99997115,\n",
       "  0.9998148,\n",
       "  0.9997378,\n",
       "  0.99994445,\n",
       "  0.9993957],\n",
       " [0.9999999,\n",
       "  0.9795359,\n",
       "  0.85792047,\n",
       "  0.982702,\n",
       "  0.9985025,\n",
       "  0.9998596,\n",
       "  0.9991984,\n",
       "  0.99947697,\n",
       "  0.9919327,\n",
       "  0.9976847],\n",
       " [1.0,\n",
       "  0.94817495,\n",
       "  0.99784744,\n",
       "  0.8479972,\n",
       "  0.8451617,\n",
       "  0.9653615,\n",
       "  0.8392821,\n",
       "  0.71830577,\n",
       "  0.9638799,\n",
       "  0.9745988],\n",
       " [8.190024e-06,\n",
       "  0.9968104,\n",
       "  0.9973514,\n",
       "  0.99708086,\n",
       "  0.99899036,\n",
       "  0.9979802,\n",
       "  0.9883372,\n",
       "  0.9997559,\n",
       "  0.96923155,\n",
       "  0.9691204],\n",
       " [1.0,\n",
       "  0.9999964,\n",
       "  0.99999774,\n",
       "  0.99999654,\n",
       "  0.99998784,\n",
       "  0.999948,\n",
       "  0.99999475,\n",
       "  0.9999949,\n",
       "  0.9999987,\n",
       "  0.9990909],\n",
       " [1.0,\n",
       "  0.70068276,\n",
       "  0.9705649,\n",
       "  0.9983658,\n",
       "  0.9888558,\n",
       "  0.994716,\n",
       "  0.9744281,\n",
       "  0.9631516,\n",
       "  0.9989447,\n",
       "  0.9838393],\n",
       " [0.0,\n",
       "  0.807637,\n",
       "  0.7719165,\n",
       "  0.4372238,\n",
       "  0.96043485,\n",
       "  0.9519611,\n",
       "  0.9980361,\n",
       "  0.9963981,\n",
       "  0.9697995,\n",
       "  0.795678],\n",
       " [0.0,\n",
       "  0.9999491,\n",
       "  0.9974953,\n",
       "  0.99957186,\n",
       "  0.99872714,\n",
       "  0.94173604,\n",
       "  0.9525473,\n",
       "  0.9998012,\n",
       "  0.999741,\n",
       "  0.9424936],\n",
       " [1.0,\n",
       "  0.840895,\n",
       "  0.98941714,\n",
       "  0.9633478,\n",
       "  0.9258484,\n",
       "  0.9779628,\n",
       "  0.97807956,\n",
       "  0.8821356,\n",
       "  0.96686804,\n",
       "  0.7396246],\n",
       " [1.0,\n",
       "  0.9992866,\n",
       "  0.9684572,\n",
       "  0.99865144,\n",
       "  0.9993426,\n",
       "  0.9988582,\n",
       "  0.95796025,\n",
       "  0.99853146,\n",
       "  0.99947625,\n",
       "  0.98134387],\n",
       " [1.0,\n",
       "  0.9999423,\n",
       "  0.9999869,\n",
       "  0.99972194,\n",
       "  0.9995895,\n",
       "  0.9995727,\n",
       "  0.99992025,\n",
       "  0.99992144,\n",
       "  0.99985874,\n",
       "  0.9996141],\n",
       " [0.0,\n",
       "  0.70146126,\n",
       "  0.13187891,\n",
       "  0.897838,\n",
       "  0.6294656,\n",
       "  0.46094668,\n",
       "  0.6431951,\n",
       "  0.055370703,\n",
       "  0.80657816,\n",
       "  0.93200886],\n",
       " [1.0,\n",
       "  0.8991173,\n",
       "  0.93665165,\n",
       "  0.98987377,\n",
       "  0.88718086,\n",
       "  0.9042631,\n",
       "  0.97636056,\n",
       "  0.9716603,\n",
       "  0.97501016,\n",
       "  0.98708165],\n",
       " [1.0,\n",
       "  0.92816544,\n",
       "  0.9918065,\n",
       "  0.16893543,\n",
       "  0.98721355,\n",
       "  0.9827443,\n",
       "  0.9986266,\n",
       "  0.9858657,\n",
       "  0.9720211,\n",
       "  0.9979006],\n",
       " [7.924444e-11,\n",
       "  0.9999999,\n",
       "  0.99999976,\n",
       "  0.99999845,\n",
       "  0.99999595,\n",
       "  0.9999671,\n",
       "  0.9999999,\n",
       "  0.99999595,\n",
       "  0.99999464,\n",
       "  0.99999416],\n",
       " [1.0,\n",
       "  0.9995658,\n",
       "  0.99981683,\n",
       "  0.99999356,\n",
       "  0.999869,\n",
       "  0.9997342,\n",
       "  0.99973875,\n",
       "  0.99761945,\n",
       "  0.99974793,\n",
       "  0.9994135],\n",
       " [9.114595e-30,\n",
       "  0.9999987,\n",
       "  0.9999927,\n",
       "  0.9999869,\n",
       "  0.99995136,\n",
       "  0.9999491,\n",
       "  0.9999976,\n",
       "  0.9999951,\n",
       "  0.9999492,\n",
       "  0.99996316],\n",
       " [1.0,\n",
       "  0.9999995,\n",
       "  0.9999858,\n",
       "  0.9997608,\n",
       "  0.9998977,\n",
       "  0.999995,\n",
       "  0.99995804,\n",
       "  0.9999864,\n",
       "  0.99960035,\n",
       "  0.9999802],\n",
       " [1.0,\n",
       "  0.99999964,\n",
       "  0.99999857,\n",
       "  0.9999821,\n",
       "  0.9998938,\n",
       "  0.99999034,\n",
       "  0.99995553,\n",
       "  0.99999774,\n",
       "  0.9998965,\n",
       "  0.9999771],\n",
       " [0.0,\n",
       "  0.9999963,\n",
       "  0.999998,\n",
       "  0.9999981,\n",
       "  0.99997735,\n",
       "  0.9998838,\n",
       "  0.9999738,\n",
       "  0.99995136,\n",
       "  0.9999927,\n",
       "  0.99937135],\n",
       " [0.0,\n",
       "  0.9685249,\n",
       "  0.22052272,\n",
       "  0.986294,\n",
       "  0.9934598,\n",
       "  0.996157,\n",
       "  0.87678057,\n",
       "  0.99286884,\n",
       "  0.8873675,\n",
       "  0.9866274],\n",
       " [1.0,\n",
       "  0.9993255,\n",
       "  0.9993511,\n",
       "  0.999998,\n",
       "  0.99991465,\n",
       "  0.99999225,\n",
       "  0.99864393,\n",
       "  0.99989426,\n",
       "  0.99993646,\n",
       "  0.99993455],\n",
       " [1.0,\n",
       "  0.99999976,\n",
       "  0.9999962,\n",
       "  0.9999895,\n",
       "  0.99990296,\n",
       "  0.9999976,\n",
       "  0.9999707,\n",
       "  0.99987423,\n",
       "  0.99998665,\n",
       "  0.9998753],\n",
       " [1.0,\n",
       "  0.9666126,\n",
       "  0.9994192,\n",
       "  0.99997044,\n",
       "  0.9999974,\n",
       "  0.99989617,\n",
       "  0.9996468,\n",
       "  0.99985754,\n",
       "  0.99957293,\n",
       "  0.99996364],\n",
       " [0.99999905,\n",
       "  0.9999963,\n",
       "  0.9999728,\n",
       "  0.99999964,\n",
       "  0.99989605,\n",
       "  0.9999882,\n",
       "  0.99991477,\n",
       "  0.99996936,\n",
       "  0.9999306,\n",
       "  0.9999095],\n",
       " [1.0,\n",
       "  0.9999999,\n",
       "  0.99999976,\n",
       "  0.99999607,\n",
       "  0.99999976,\n",
       "  0.9999678,\n",
       "  0.99999976,\n",
       "  0.99999976,\n",
       "  0.99999046,\n",
       "  0.99999845],\n",
       " [2.9303528e-15,\n",
       "  0.99915516,\n",
       "  0.998021,\n",
       "  0.9885018,\n",
       "  0.85117465,\n",
       "  0.9991903,\n",
       "  0.9151327,\n",
       "  0.99940884,\n",
       "  0.9970414,\n",
       "  0.93484646],\n",
       " [1.0,\n",
       "  0.9991677,\n",
       "  0.99844736,\n",
       "  0.998698,\n",
       "  0.98985475,\n",
       "  0.9996847,\n",
       "  0.9954808,\n",
       "  0.9973617,\n",
       "  0.99095297,\n",
       "  0.9924832],\n",
       " [1.0,\n",
       "  0.9999752,\n",
       "  0.99997103,\n",
       "  0.9999987,\n",
       "  0.99999964,\n",
       "  1.0,\n",
       "  0.99978787,\n",
       "  1.0,\n",
       "  0.9999939,\n",
       "  0.99999356],\n",
       " [1.0,\n",
       "  0.9984485,\n",
       "  0.99956614,\n",
       "  0.9889543,\n",
       "  0.7862958,\n",
       "  0.9991061,\n",
       "  0.94310564,\n",
       "  0.9993723,\n",
       "  0.9972984,\n",
       "  0.98863286],\n",
       " [1.0,\n",
       "  0.999998,\n",
       "  0.9999962,\n",
       "  0.99999154,\n",
       "  0.9999021,\n",
       "  0.99999857,\n",
       "  0.9999776,\n",
       "  0.99926466,\n",
       "  0.9997514,\n",
       "  0.999936],\n",
       " [0.0,\n",
       "  0.75155425,\n",
       "  0.79933953,\n",
       "  0.9440677,\n",
       "  0.99850804,\n",
       "  0.96734846,\n",
       "  0.995368,\n",
       "  0.9970331,\n",
       "  0.99906296,\n",
       "  0.99942774],\n",
       " [1.0,\n",
       "  0.9998043,\n",
       "  0.9999887,\n",
       "  0.99949205,\n",
       "  0.9990101,\n",
       "  0.99996233,\n",
       "  0.999582,\n",
       "  0.9995808,\n",
       "  0.9998385,\n",
       "  0.9998234],\n",
       " [1.0,\n",
       "  0.9985991,\n",
       "  0.999121,\n",
       "  0.99918,\n",
       "  0.99943167,\n",
       "  0.99999356,\n",
       "  0.99349916,\n",
       "  0.9990521,\n",
       "  0.9997342,\n",
       "  0.99941814],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  0.99935514,\n",
       "  0.9853264,\n",
       "  0.99999404,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999987,\n",
       "  0.9999956,\n",
       "  0.99999166],\n",
       " [1.0,\n",
       "  0.99849904,\n",
       "  0.9994074,\n",
       "  0.9999603,\n",
       "  0.9999982,\n",
       "  0.99993384,\n",
       "  0.99955016,\n",
       "  0.9998192,\n",
       "  0.99971646,\n",
       "  0.999984],\n",
       " [0.0,\n",
       "  0.46813723,\n",
       "  0.22825038,\n",
       "  0.32328498,\n",
       "  0.23078135,\n",
       "  0.10448336,\n",
       "  0.1166286,\n",
       "  0.22649215,\n",
       "  0.27708796,\n",
       "  0.78389794],\n",
       " [1.0,\n",
       "  0.99007624,\n",
       "  0.9609758,\n",
       "  0.9994615,\n",
       "  0.96264124,\n",
       "  0.9946596,\n",
       "  0.9991309,\n",
       "  0.9380277,\n",
       "  0.9955859,\n",
       "  0.99791807],\n",
       " [1.0,\n",
       "  0.37922564,\n",
       "  0.59541464,\n",
       "  0.28494853,\n",
       "  0.73541856,\n",
       "  0.9299259,\n",
       "  0.0071515054,\n",
       "  0.45118493,\n",
       "  0.87885964,\n",
       "  0.8599994],\n",
       " [3.3118652e-19,\n",
       "  0.99996746,\n",
       "  0.9999912,\n",
       "  0.99995613,\n",
       "  0.99487513,\n",
       "  0.9997472,\n",
       "  0.99999917,\n",
       "  0.9999398,\n",
       "  0.9998628,\n",
       "  0.9996984],\n",
       " [0.0,\n",
       "  0.9997162,\n",
       "  0.99162674,\n",
       "  0.98767567,\n",
       "  0.9854637,\n",
       "  0.7180038,\n",
       "  0.9958138,\n",
       "  0.9910821,\n",
       "  0.98425484,\n",
       "  0.94134015],\n",
       " [1.0,\n",
       "  0.99869746,\n",
       "  0.99995077,\n",
       "  0.9927037,\n",
       "  0.9976338,\n",
       "  0.99932754,\n",
       "  0.99964595,\n",
       "  0.9975815,\n",
       "  0.9970504,\n",
       "  0.998744],\n",
       " [1.0,\n",
       "  0.99987316,\n",
       "  0.99999535,\n",
       "  0.99999,\n",
       "  0.9997836,\n",
       "  0.9999012,\n",
       "  0.9998958,\n",
       "  0.99953973,\n",
       "  0.99999034,\n",
       "  0.99999547],\n",
       " [1.0,\n",
       "  0.9999949,\n",
       "  0.9999726,\n",
       "  0.9994704,\n",
       "  0.99987614,\n",
       "  0.999273,\n",
       "  0.9999882,\n",
       "  0.9999658,\n",
       "  0.9998919,\n",
       "  0.9999924],\n",
       " [1.0,\n",
       "  0.9365388,\n",
       "  0.9987828,\n",
       "  0.999866,\n",
       "  0.99997413,\n",
       "  0.99963677,\n",
       "  0.9898498,\n",
       "  0.9996619,\n",
       "  0.9997123,\n",
       "  0.9999336],\n",
       " [1.0,\n",
       "  0.99387485,\n",
       "  0.9989988,\n",
       "  0.746325,\n",
       "  0.938617,\n",
       "  0.33019555,\n",
       "  0.9146777,\n",
       "  0.95896834,\n",
       "  0.6516622,\n",
       "  0.99990785],\n",
       " [1.0,\n",
       "  0.89618903,\n",
       "  0.98881835,\n",
       "  0.9394314,\n",
       "  0.989349,\n",
       "  0.79808027,\n",
       "  0.99153954,\n",
       "  0.93129104,\n",
       "  0.9378907,\n",
       "  0.9922724],\n",
       " [1.0,\n",
       "  0.9997545,\n",
       "  0.99999464,\n",
       "  0.9999894,\n",
       "  0.99999404,\n",
       "  0.9999095,\n",
       "  0.9998179,\n",
       "  0.9999906,\n",
       "  0.9995472,\n",
       "  0.99265623],\n",
       " [1.0,\n",
       "  0.99999964,\n",
       "  0.9999989,\n",
       "  0.999985,\n",
       "  0.9999267,\n",
       "  0.99959844,\n",
       "  0.99995196,\n",
       "  0.99999905,\n",
       "  0.9998841,\n",
       "  0.99998367],\n",
       " [1.0, 1.0, 1.0, 1.0, 1.0, 0.9999994, 1.0, 1.0, 0.99999964, 1.0],\n",
       " [1.0,\n",
       "  0.99987864,\n",
       "  0.99999464,\n",
       "  0.9999999,\n",
       "  0.99989665,\n",
       "  0.9997358,\n",
       "  0.99997044,\n",
       "  0.99999976,\n",
       "  0.9999926,\n",
       "  0.9995316],\n",
       " [1.0,\n",
       "  0.99999034,\n",
       "  0.99969006,\n",
       "  0.99173355,\n",
       "  0.9999716,\n",
       "  0.9987703,\n",
       "  0.9995453,\n",
       "  0.999292,\n",
       "  0.9996345,\n",
       "  0.99975413],\n",
       " [0.0,\n",
       "  0.99973065,\n",
       "  0.9998029,\n",
       "  0.9999778,\n",
       "  0.999539,\n",
       "  0.97650814,\n",
       "  0.9994698,\n",
       "  0.9998265,\n",
       "  0.99988973,\n",
       "  0.9860931],\n",
       " [1.0,\n",
       "  0.99531144,\n",
       "  0.9978397,\n",
       "  0.9945044,\n",
       "  0.99872833,\n",
       "  0.99714077,\n",
       "  0.99953985,\n",
       "  0.99947256,\n",
       "  0.9591397,\n",
       "  0.9964946],\n",
       " [3.3314912e-20,\n",
       "  0.95113367,\n",
       "  0.971162,\n",
       "  0.95512336,\n",
       "  0.995961,\n",
       "  0.99790335,\n",
       "  0.99950135,\n",
       "  0.9968119,\n",
       "  0.99848074,\n",
       "  0.9990458],\n",
       " [1.0,\n",
       "  0.9999993,\n",
       "  0.9999994,\n",
       "  0.999987,\n",
       "  0.9999813,\n",
       "  0.9999989,\n",
       "  0.9999927,\n",
       "  0.99994206,\n",
       "  0.99999714,\n",
       "  0.9999777],\n",
       " [1.0,\n",
       "  0.9997079,\n",
       "  0.9999726,\n",
       "  0.9933171,\n",
       "  0.99997413,\n",
       "  0.9994898,\n",
       "  0.99974734,\n",
       "  0.9977089,\n",
       "  0.99743015,\n",
       "  0.99968064],\n",
       " [1.0,\n",
       "  0.99868494,\n",
       "  0.9998134,\n",
       "  0.9967139,\n",
       "  0.9995117,\n",
       "  0.9977094,\n",
       "  0.9939308,\n",
       "  0.99908924,\n",
       "  0.99793893,\n",
       "  0.9968857],\n",
       " [1.0,\n",
       "  0.9908366,\n",
       "  0.7840831,\n",
       "  0.989758,\n",
       "  0.99880683,\n",
       "  0.99901545,\n",
       "  0.99770087,\n",
       "  0.99994135,\n",
       "  0.9989574,\n",
       "  0.99951124],\n",
       " [6.9552224e-12,\n",
       "  0.99974793,\n",
       "  0.19589034,\n",
       "  0.86846334,\n",
       "  0.971649,\n",
       "  0.9999217,\n",
       "  0.9999894,\n",
       "  0.99976856,\n",
       "  0.97584116,\n",
       "  0.98271066],\n",
       " [1.0,\n",
       "  0.9992217,\n",
       "  0.998923,\n",
       "  0.9985972,\n",
       "  0.98502666,\n",
       "  0.99403363,\n",
       "  0.9990414,\n",
       "  0.9847741,\n",
       "  0.9959706,\n",
       "  0.9867808],\n",
       " [1.0,\n",
       "  0.9999106,\n",
       "  0.9977496,\n",
       "  0.9995011,\n",
       "  0.99988174,\n",
       "  0.9999993,\n",
       "  0.9999794,\n",
       "  0.99999404,\n",
       "  0.99985576,\n",
       "  0.99881446],\n",
       " [0.0,\n",
       "  0.41826937,\n",
       "  0.6847404,\n",
       "  0.7493434,\n",
       "  0.80841935,\n",
       "  0.66824305,\n",
       "  0.85310864,\n",
       "  0.90176135,\n",
       "  0.89773077,\n",
       "  0.86287636],\n",
       " [1.0,\n",
       "  0.9999585,\n",
       "  0.9999126,\n",
       "  0.9996853,\n",
       "  0.99933714,\n",
       "  0.99992,\n",
       "  0.99970776,\n",
       "  0.9990544,\n",
       "  0.99962187,\n",
       "  0.9988393],\n",
       " [1.0,\n",
       "  0.350633,\n",
       "  0.99080783,\n",
       "  0.9833649,\n",
       "  0.9825665,\n",
       "  0.9985422,\n",
       "  0.9749678,\n",
       "  0.98699796,\n",
       "  0.99768865,\n",
       "  0.9909116],\n",
       " [1.0,\n",
       "  0.9999994,\n",
       "  0.9999969,\n",
       "  0.99999154,\n",
       "  0.99992406,\n",
       "  0.99999666,\n",
       "  0.99999285,\n",
       "  0.999926,\n",
       "  0.99992895,\n",
       "  0.9998405],\n",
       " [1.0,\n",
       "  0.996068,\n",
       "  0.9998789,\n",
       "  0.99999154,\n",
       "  0.99995613,\n",
       "  0.9999858,\n",
       "  0.98313165,\n",
       "  0.998548,\n",
       "  0.9994591,\n",
       "  0.9998441],\n",
       " [1.0,\n",
       "  0.99997556,\n",
       "  0.99998546,\n",
       "  0.9995553,\n",
       "  0.9975914,\n",
       "  0.999859,\n",
       "  0.99997413,\n",
       "  0.9989009,\n",
       "  0.9988029,\n",
       "  0.9991365],\n",
       " [1.0,\n",
       "  0.9737694,\n",
       "  0.99906224,\n",
       "  0.99991393,\n",
       "  0.9997298,\n",
       "  0.99997675,\n",
       "  0.9827946,\n",
       "  0.9994629,\n",
       "  0.9996314,\n",
       "  0.9989814],\n",
       " [0.0,\n",
       "  0.99996805,\n",
       "  0.9994023,\n",
       "  0.9911902,\n",
       "  0.996128,\n",
       "  0.99885476,\n",
       "  0.9999105,\n",
       "  0.9979857,\n",
       "  0.9941935,\n",
       "  0.9993765],\n",
       " [1.0,\n",
       "  1.0,\n",
       "  0.9999995,\n",
       "  0.9999995,\n",
       "  0.99999905,\n",
       "  1.0,\n",
       "  0.9999994,\n",
       "  0.99999976,\n",
       "  0.9999957,\n",
       "  0.99999833],\n",
       " [1.0,\n",
       "  0.47565535,\n",
       "  0.92638355,\n",
       "  0.8017405,\n",
       "  0.9763313,\n",
       "  0.99942744,\n",
       "  0.9690017,\n",
       "  0.98842573,\n",
       "  0.9844481,\n",
       "  0.9674893],\n",
       " [1.1176652e-05,\n",
       "  0.4882338,\n",
       "  0.6637581,\n",
       "  0.99713194,\n",
       "  0.927133,\n",
       "  0.9937568,\n",
       "  0.960269,\n",
       "  0.81999725,\n",
       "  0.99364835,\n",
       "  0.98525375],\n",
       " [1.0,\n",
       "  0.9997003,\n",
       "  0.6477725,\n",
       "  0.8459842,\n",
       "  0.9992619,\n",
       "  0.999997,\n",
       "  0.99998844,\n",
       "  0.99998534,\n",
       "  0.99942243,\n",
       "  0.9992816],\n",
       " [0.0,\n",
       "  0.9987729,\n",
       "  0.9756107,\n",
       "  0.9899636,\n",
       "  0.9987924,\n",
       "  0.9124247,\n",
       "  0.99551445,\n",
       "  0.92177165,\n",
       "  0.9673519,\n",
       "  0.88199383],\n",
       " [1.0, 1.0, 0.9999994, 1.0, 0.9999995, 1.0, 1.0, 0.99999976, 0.9999995, 1.0],\n",
       " [1.0,\n",
       "  0.9982938,\n",
       "  0.998858,\n",
       "  0.98715216,\n",
       "  0.97269213,\n",
       "  0.99941885,\n",
       "  0.9965078,\n",
       "  0.9978429,\n",
       "  0.9963864,\n",
       "  0.99401075],\n",
       " [0.0,\n",
       "  0.999871,\n",
       "  0.99979216,\n",
       "  0.99973017,\n",
       "  0.99338746,\n",
       "  0.9999877,\n",
       "  0.99948394,\n",
       "  0.99581665,\n",
       "  0.9982344,\n",
       "  0.9998983],\n",
       " [0.0,\n",
       "  0.9900709,\n",
       "  0.9992508,\n",
       "  0.96363574,\n",
       "  0.9535981,\n",
       "  0.99280363,\n",
       "  0.99841726,\n",
       "  0.9976597,\n",
       "  0.9814973,\n",
       "  0.9739343],\n",
       " [1.0,\n",
       "  0.99997544,\n",
       "  0.9997234,\n",
       "  0.9999763,\n",
       "  0.9999871,\n",
       "  0.99999785,\n",
       "  0.99949145,\n",
       "  0.99981135,\n",
       "  0.99905115,\n",
       "  0.9999981],\n",
       " [1.0,\n",
       "  0.31331104,\n",
       "  0.2766196,\n",
       "  0.8150796,\n",
       "  0.9130413,\n",
       "  0.554486,\n",
       "  0.751849,\n",
       "  0.39270195,\n",
       "  0.67885584,\n",
       "  0.32303932],\n",
       " [1.0,\n",
       "  0.99138135,\n",
       "  0.99847597,\n",
       "  0.9999807,\n",
       "  0.99998224,\n",
       "  0.9997831,\n",
       "  0.99974436,\n",
       "  0.99597436,\n",
       "  0.9994185,\n",
       "  0.9998975],\n",
       " [0.0,\n",
       "  0.42546377,\n",
       "  0.9713212,\n",
       "  0.24892622,\n",
       "  0.39477545,\n",
       "  0.49618378,\n",
       "  0.96091586,\n",
       "  0.81645596,\n",
       "  0.74755204,\n",
       "  0.79189783],\n",
       " [1.0,\n",
       "  0.999998,\n",
       "  0.9999827,\n",
       "  0.9998702,\n",
       "  0.99968266,\n",
       "  0.9999888,\n",
       "  0.99937755,\n",
       "  0.99524117,\n",
       "  0.99992466,\n",
       "  0.99992824],\n",
       " [1.0,\n",
       "  0.9999981,\n",
       "  0.9999988,\n",
       "  0.9999964,\n",
       "  0.9999937,\n",
       "  0.99996495,\n",
       "  0.9998708,\n",
       "  0.9999987,\n",
       "  0.9999962,\n",
       "  0.9999857],\n",
       " [4.6597478e-34,\n",
       "  0.99825376,\n",
       "  0.99874985,\n",
       "  0.9995828,\n",
       "  0.9997664,\n",
       "  0.99774015,\n",
       "  0.99782455,\n",
       "  0.99997497,\n",
       "  0.99830806,\n",
       "  0.9820738],\n",
       " [1.0,\n",
       "  0.99228364,\n",
       "  0.5148159,\n",
       "  0.23879552,\n",
       "  0.99843115,\n",
       "  0.99978095,\n",
       "  0.9988637,\n",
       "  0.999977,\n",
       "  0.9996569,\n",
       "  0.92824143]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_probabilities_for_most_frequent_class(client_models, x_test, most_frequent_classes):\n",
    "    # List to store the probabilities for each sample\n",
    "    all_probabilities = []\n",
    "\n",
    "    # Iterate over each sample and the corresponding most frequent class\n",
    "    for sample_idx, temp_sample in enumerate(x_test):\n",
    "        sample_probabilities = []\n",
    "        most_frequent_class = most_frequent_classes[sample_idx]  # The most frequent class for this sample\n",
    "        \n",
    "        # For each client model, extract the probability for the most frequent class\n",
    "        for temp_client_model in client_models:\n",
    "            predicted_probs = temp_client_model.predict(np.expand_dims(temp_sample, axis=0))[0]\n",
    "            probability_for_most_frequent_class = predicted_probs[most_frequent_class]  # Get the probability for the class\n",
    "            sample_probabilities.append(probability_for_most_frequent_class)\n",
    "        \n",
    "        # Store the probabilities for this sample across all models\n",
    "        all_probabilities.append(sample_probabilities)\n",
    "\n",
    "    return all_probabilities\n",
    "\n",
    "# Extract probabilities for the most frequent class for each sample\n",
    "probabilities_for_most_frequent_class = extract_probabilities_for_most_frequent_class(client_models_m, x_test_limited, most_frequent_classes_list)\n",
    "print(len(probabilities_for_most_frequent_class[1]))\n",
    "probabilities_for_most_frequent_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9363ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Distance Matrix:\n",
      "[[0.    0.735 0.755 0.745 0.755 0.73  0.74  0.745 0.755 0.75 ]\n",
      " [0.735 0.    0.065 0.07  0.09  0.095 0.1   0.075 0.11  0.115]\n",
      " [0.755 0.065 0.    0.05  0.07  0.075 0.085 0.075 0.08  0.095]\n",
      " [0.745 0.07  0.05  0.    0.075 0.085 0.085 0.065 0.09  0.09 ]\n",
      " [0.755 0.09  0.07  0.075 0.    0.105 0.07  0.055 0.065 0.08 ]\n",
      " [0.73  0.095 0.075 0.085 0.105 0.    0.125 0.09  0.125 0.13 ]\n",
      " [0.74  0.1   0.085 0.085 0.07  0.125 0.    0.065 0.055 0.065]\n",
      " [0.745 0.075 0.075 0.065 0.055 0.09  0.065 0.    0.065 0.075]\n",
      " [0.755 0.11  0.08  0.09  0.065 0.125 0.055 0.065 0.    0.09 ]\n",
      " [0.75  0.115 0.095 0.09  0.08  0.13  0.065 0.075 0.09  0.   ]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_test_between_clients(probabilities):\n",
    "    num_clients = len(probabilities[0])  # Number of client models\n",
    "    ks_distance_matrix = np.zeros((num_clients, num_clients))  # Initialize distance matrix\n",
    "\n",
    "    # Iterate over each pair of clients (i, j)\n",
    "    for i in range(num_clients):\n",
    "        for j in range(i + 1, num_clients):\n",
    "            # Collect the probabilities for client i and client j\n",
    "            probs_client_i = [prob[i] for prob in probabilities]\n",
    "            probs_client_j = [prob[j] for prob in probabilities]\n",
    "            \n",
    "            # Perform the KS test between the two sets of probabilities\n",
    "            ks_statistic, _ = ks_2samp(probs_client_i, probs_client_j)\n",
    "            \n",
    "            # Store the KS statistic (D value) in the distance matrix\n",
    "            ks_distance_matrix[i, j] = ks_statistic\n",
    "            ks_distance_matrix[j, i] = ks_statistic  # Symmetric matrix\n",
    "\n",
    "    return ks_distance_matrix\n",
    "\n",
    "# Perform the KS test between clients\n",
    "ks_distance_matrix = ks_test_between_clients(probabilities_for_most_frequent_class)\n",
    "\n",
    "# Print the KS distance matrix\n",
    "print(\"KS Distance Matrix:\")\n",
    "print(ks_distance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5ad2aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers detected: []\n"
     ]
    }
   ],
   "source": [
    "# Use IQR to detect outliers in the KS distance matrix\n",
    "ks_mean_distances = meancal(ks_distance_matrix)\n",
    "outliers = iqrfunc(ks_mean_distances)\n",
    "\n",
    "print(\"Outliers detected:\", outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a582ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1acda574",
   "metadata": {},
   "source": [
    "## chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bab56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(model1, model2, x_test):\n",
    "    # Make predictions on the test data\n",
    "    predictions1 = np.argmax(model1.predict(x_test), axis=1)\n",
    "    predictions2 = np.argmax(model2.predict(x_test), axis=1)\n",
    "    \n",
    "    # Create a contingency table\n",
    "    contingency_table = np.zeros((10, 10))\n",
    "    for i in range(len(predictions1)):\n",
    "        contingency_table[predictions1[i], predictions2[i]] += 1\n",
    "    \n",
    "    # Perform Chi-Square test\n",
    "    chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "    \n",
    "    return chi2_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88617b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test Statistic Matrix:\n",
      "[[    0.         58631.32448507 59020.23089166 74907.01631594\n",
      "  59988.94075193 60675.94848906 73582.50822956 59410.20151447\n",
      "  59605.71404062 74020.55458926]\n",
      " [58631.32448507     0.         77724.63242927 60697.06459551\n",
      "  77960.6171941  77358.44624229 60677.22905001 78659.28278013\n",
      "  79476.48782308 59298.87634242]\n",
      " [59020.23089166 77724.63242927     0.         61614.00798636\n",
      "  78365.35110997 78069.13182678 60708.55991072 78410.42800881\n",
      "  78682.50117303 59331.88834187]\n",
      " [74907.01631594 60697.06459551 61614.00798636     0.\n",
      "  62786.20008506 63136.41747253 74355.09129194 62047.43467581\n",
      "  62034.92189424 75767.33836208]\n",
      " [59988.94075193 77960.6171941  78365.35110997 62786.20008506\n",
      "      0.         80940.60615414 61560.01332318 80880.75471021\n",
      "  80768.19365113 60710.82044645]\n",
      " [60675.94848906 77358.44624229 78069.13182678 63136.41747253\n",
      "  80940.60615414     0.         62205.10289165 80145.91619682\n",
      "  79772.64394083 61324.08159264]\n",
      " [73582.50822956 60677.22905001 60708.55991072 74355.09129194\n",
      "  61560.01332318 62205.10289165     0.         61495.80348089\n",
      "  61476.35924089 74913.69421729]\n",
      " [59410.20151447 78659.28278013 78410.42800881 62047.43467581\n",
      "  80880.75471021 80145.91619682 61495.80348089     0.\n",
      "  80805.23606486 60040.38706694]\n",
      " [59605.71404062 79476.48782308 78682.50117303 62034.92189424\n",
      "  80768.19365113 79772.64394083 61476.35924089 80805.23606486\n",
      "      0.         60125.83893124]\n",
      " [74020.55458926 59298.87634242 59331.88834187 75767.33836208\n",
      "  60710.82044645 61324.08159264 74913.69421729 60040.38706694\n",
      "  60125.83893124     0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a matrix to store Chi-Square test results\n",
    "num_clients = len(client_models_m)\n",
    "chi2_stat_matrix = np.zeros((num_clients, num_clients))\n",
    "\n",
    "# Calculate the Chi-Square test for each pair of models\n",
    "for i in range(num_clients):\n",
    "    for j in range(i + 1, num_clients):\n",
    "        chi2_stat = chi_square_test(client_models_m[i], client_models_m[j], x_test_m)\n",
    "        chi2_stat_matrix[i, j] = chi2_stat\n",
    "        chi2_stat_matrix[j, i] = chi2_stat  # Symmetric matrix\n",
    "\n",
    "# Print the Chi-Square test statistic matrix\n",
    "print(\"Chi-Square Test Statistic Matrix:\")\n",
    "print(chi2_stat_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebd6b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 5, 6, 9], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meancal(matrix):\n",
    "    temp = 0\n",
    "    x = matrix.shape[0]    \n",
    "    arrmean = []\n",
    "    \n",
    "    for i in range(0,x):\n",
    "        #print(matrix[i].mean())\n",
    "        temp = (matrix[i].mean())\n",
    "        arrmean.append(temp)\n",
    "    return arrmean\n",
    "temp = meancal(distance_matrix)\n",
    "def iqrfunc(nparray, multiplier = 1.5):\n",
    "    data = np.array(nparray)\n",
    "    q1 = np.percentile(data,25)\n",
    "    q3 = np.percentile(data,75)\n",
    "    iqr = q3 -q1\n",
    "    lower_bound = q1-(multiplier*1.5)\n",
    "    upper_bound = q3+(multiplier*1.5)\n",
    "    outliers = np.where((data<lower_bound) | (data>upper_bound))[0]\n",
    "    return outliers\n",
    "                        \n",
    "iqrfunc(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5796fea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cf429c-9f38-4005-8489-0dd34c1c9393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ایجاد پلات\n",
    "# تعریف تعداد کلاینت‌ها به صورت دینامیک\n",
    "num_models = len(client_data_m)  # یا اگر از num_models استفاده می‌کنید: num_clients = num_models\n",
    "clients = [f'Client {i+1}' for i in range(num_models)]  # ایجاد نام کلاینت‌ها به صورت خودکار\n",
    "\n",
    "x = np.arange(len(clients))  # محور x\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# ایجاد دو نمودار میله‌ای برای دقت‌ها\n",
    "plt.bar(x - 0.2, accuracy_no_bug, 0.4, label='No Bug', color='skyblue')\n",
    "plt.bar(x + 0.2, accuracy_with_bug, 0.4, label='With Bug (Client 3)', color='salmon')\n",
    "\n",
    "# برچسب‌ها و عنوان\n",
    "plt.xlabel('Clients')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy Comparison: With vs Without Bug in Client 3')\n",
    "plt.xticks(x, clients)\n",
    "plt.ylim(0.6, 1.0)  # محدوده دقت بین 0.6 تا 1.0\n",
    "\n",
    "# نمایش راهنما و پلات\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ee719d-d575-4c04-878e-225beee917a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_class(models, x_test):\n",
    "    num_models = len(models)\n",
    "    preds = [model.predict(x_test).argmax(axis=1) for model in models]         # پیش‌بینی‌ها برای هر مدل\n",
    "    diffs_matrix = np.zeros((num_models, num_models), dtype=int)    # ماتریس مربعی برای ذخیره تفاوت‌ها\n",
    "    for i in range(num_models):       # پر کردن ماتریس با تفاوت پیش‌بینی‌ها\n",
    "        for j in range(i+1, num_models):\n",
    "            diffs = np.sum(preds[i] != preds[j])  # تعداد تفاوت‌های پیش‌بینی بین مدل i و مدل j\n",
    "            diffs_matrix[i, j] = diffs\n",
    "            diffs_matrix[j, i] = diffs  # ماتریس متقارن است\n",
    "    return diffs_matrix\n",
    "\n",
    "def delta_score(models, x_test):\n",
    "    num_models = len(models)\n",
    "    preds = [model.predict(x_test) for model in models]     # پیش‌بینی‌ها برای هر مدل\n",
    "    diffs_matrix = np.zeros((num_models, num_models))      # ماتریس تفاوت‌ها  \n",
    "    for i in range(num_models):      # محاسبه تفاوت‌ها برای هر جفت مدل\n",
    "        for j in range(i+1, num_models):\n",
    "            diffs = np.mean(np.abs(preds[i] - preds[j]))  # میانگین تفاوت مطلق\n",
    "            diffs_matrix[i, j] = diffs\n",
    "            diffs_matrix[j, i] = diffs  # متقارن\n",
    "    return diffs_matrix\n",
    "\n",
    "def p_ks(models, x_test):\n",
    "    num_models = len(models)      \n",
    "    scores = [model.predict(x_test) for model in models]       # امتیازات پیش‌بینی‌ها برای هر مدل   \n",
    "    ks_matrix = np.zeros((num_models, num_models))  # ماتریس برای آماره آزمون KS و p-value\n",
    "    pvalue_matrix = np.zeros((num_models, num_models))    \n",
    "    # محاسبه آزمون KS برای هر جفت مدل\n",
    "    for i in range(num_models):\n",
    "        for j in range(i+1, num_models):          \n",
    "            ks_stat, p_value = stats.ks_2samp(scores[i].flatten(), scores[j].flatten())  # محاسبه آزمون KS و دریافت آماره و p-value      \n",
    "            # چک کردن برای جلوگیری از NaN یا Inf\n",
    "            ks_stat = ks_stat if np.isfinite(ks_stat) else 0\n",
    "            p_value = p_value if np.isfinite(p_value) else 1   \n",
    "            # پر کردن ماتریس‌ها\n",
    "            ks_matrix[i, j] = ks_stat\n",
    "            ks_matrix[j, i] = ks_stat  # متقارن\n",
    "            pvalue_matrix[i, j] = p_value\n",
    "            pvalue_matrix[j, i] = p_value  # متقارن   \n",
    "    return ks_matrix, pvalue_matrix\n",
    "\n",
    "def p_x2(models, x_test):\n",
    "    num_models = len(models)    \n",
    "    # پیش‌بینی کلاس‌ها برای هر مدل\n",
    "    preds = [model.predict(x_test).argmax(axis=1) for model in models]    \n",
    "    p_x2 = np.zeros((num_models, num_models))  # ماتریس برای آماره آزمون کای دو و p-value\n",
    "    pvalue_matrix = np.zeros((num_models, num_models))    \n",
    "    # محاسبه آزمون کای دو برای هر جفت مدل\n",
    "    for i in range(num_models):\n",
    "        for j in range(i+1, num_models):\n",
    "            # محاسبه فراوانی کلاس‌های هر دو مدل\n",
    "            bincount_i = np.bincount(preds[i])\n",
    "            bincount_j = np.bincount(preds[j])            \n",
    "            # اطمینان از اینکه هیچ عنصری صفر نباشد (برای جلوگیری از تقسیم بر صفر)\n",
    "            bincount_i = np.where(bincount_i == 0, 1e-9, bincount_i)\n",
    "            bincount_j = np.where(bincount_j == 0, 1e-9, bincount_j)           \n",
    "            x2, p_value = stats.chisquare(bincount_i, bincount_j)         # محاسبه آزمون کای دو و دریافت آماره و p-value     \n",
    "            # چک کردن برای جلوگیری از NaN یا Inf\n",
    "            x2 = x2 if np.isfinite(x2) else 0\n",
    "            p_value = p_value if np.isfinite(p_value) else 1            \n",
    "            # پر کردن ماتریس‌ها\n",
    "            p_x2[i, j] = x2\n",
    "            p_x2[j, i] = x2  # متقارن\n",
    "            pvalue_matrix[i, j] = p_value\n",
    "            pvalue_matrix[j, i] = p_value  # متقارن    \n",
    "    return p_x2, pvalue_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f553f-2675-4db8-9ec3-ce0e7fc6a873",
   "metadata": {},
   "source": [
    "## Delta class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b019e7-3bc6-41ef-aa35-6ad49a8f2829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ِDifference Matrix For MNIST Delta Class:\n",
      "[[   0 2087 2029  805 1931 1868  867 1975 1973  863]\n",
      " [2087    0  649 1977  628  667 1968  587  543 2106]\n",
      " [2029  649    0 1893  616  644 1932  617  600 2074]\n",
      " [ 805 1977 1893    0 1792 1744  808 1841 1847  729]\n",
      " [1931  628  616 1792    0  467 1851  467  474 1950]\n",
      " [1868  667  644 1744  467    0 1779  506  528 1879]\n",
      " [ 867 1968 1932  808 1851 1779    0 1855 1867  777]\n",
      " [1975  587  617 1841  467  506 1855    0  469 1995]\n",
      " [1973  543  600 1847  474  528 1867  469    0 1998]\n",
      " [ 863 2106 2074  729 1950 1879  777 1995 1998    0]]\n"
     ]
    }
   ],
   "source": [
    "# diff_matrix_class_c = delta_class(client_models_c, x_test_c)\n",
    "diff_matrix_class_m = delta_class(client_models_m, x_test_m)\n",
    "\n",
    "# print(\"\\nDifference Matrix For CIFAR Delta Class:\")\n",
    "# print(diff_matrix_class_c)\n",
    "\n",
    "print(\"\\nِDifference Matrix For MNIST Delta Class:\")\n",
    "print(diff_matrix_class_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d420122-469a-4fbb-a1e9-38bfd3cfbfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 5, 6, 9], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meancal(matrix):\n",
    "    temp = 0\n",
    "    x = matrix.shape[0]    \n",
    "    arrmean = []\n",
    "    \n",
    "    for i in range(0,x):\n",
    "        #print(matrix[i].mean())\n",
    "        temp = ((matrix[i].mean())*10)/9\n",
    "        arrmean.append(temp)\n",
    "    return arrmean\n",
    "temp = meancal(diff_matrix_class_m)\n",
    "def iqrfunc(nparray, multiplier = 1.5):\n",
    "    data = np.array(nparray)\n",
    "    q1 = np.percentile(data,25)\n",
    "    q3 = np.percentile(data,75)\n",
    "    iqr = q3 -q1\n",
    "    lower_bound = q1-(multiplier*1.5)\n",
    "    upper_bound = q3+(multiplier*1.5)\n",
    "    outliers = np.where((data<lower_bound) | (data>upper_bound))[0]\n",
    "    return outliers\n",
    "                        \n",
    "iqrfunc(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a467d7-1de8-4cd3-b51c-c81e87a62b09",
   "metadata": {},
   "source": [
    "## Delta score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a28f6d5-eadd-44c5-a3c4-3f59c449a88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.04424161, 0.0426726 , 0.01604572, 0.04021139,\n",
       "        0.03863044, 0.01733736, 0.04113601, 0.04104029, 0.01729444],\n",
       "       [0.04424161, 0.        , 0.01503946, 0.04170459, 0.01542144,\n",
       "        0.01578955, 0.04168662, 0.01424946, 0.01299856, 0.04418224],\n",
       "       [0.0426726 , 0.01503946, 0.        , 0.03984931, 0.01471316,\n",
       "        0.01526167, 0.0407267 , 0.01419721, 0.01372406, 0.04349344],\n",
       "       [0.01604572, 0.04170459, 0.03984931, 0.        , 0.03743829,\n",
       "        0.03609448, 0.01605168, 0.03847437, 0.03846978, 0.01452601],\n",
       "       [0.04021139, 0.01542144, 0.01471316, 0.03743829, 0.        ,\n",
       "        0.01098867, 0.03860414, 0.01131845, 0.0110403 , 0.04049689],\n",
       "       [0.03863044, 0.01578955, 0.01526167, 0.03609448, 0.01098867,\n",
       "        0.        , 0.03661956, 0.01168868, 0.01170166, 0.03850681],\n",
       "       [0.01733736, 0.04168662, 0.0407267 , 0.01605168, 0.03860414,\n",
       "        0.03661956, 0.        , 0.03884878, 0.03887408, 0.01548705],\n",
       "       [0.04113601, 0.01424946, 0.01419721, 0.03847437, 0.01131845,\n",
       "        0.01168868, 0.03884878, 0.        , 0.01079185, 0.04143294],\n",
       "       [0.04104029, 0.01299856, 0.01372406, 0.03846978, 0.0110403 ,\n",
       "        0.01170166, 0.03887408, 0.01079185, 0.        , 0.04133239],\n",
       "       [0.01729444, 0.04418224, 0.04349344, 0.01452601, 0.04049689,\n",
       "        0.03850681, 0.01548705, 0.04143294, 0.04133239, 0.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_matrix_score_m = delta_score(client_models_m, x_test_m)\n",
    "diff_matrix_score_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37c03627-d75d-4808-91de-3c5c8b5416b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2 = meancal(diff_matrix_score_m)\n",
    "iqrfunc(temp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5ec74-9ceb-438b-b554-df5258d73607",
   "metadata": {},
   "source": [
    "## KS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2eee62b-b96f-478d-b0b0-24507bbb0f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_matrix_m, pvalue_matrix_m = p_ks(client_models_m, x_test_m)\n",
    "temp3 = meancal(ks_matrix_m)\n",
    "iqrfunc(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11bd78d-bee5-4e74-9f1b-4700ed4a3e40",
   "metadata": {},
   "source": [
    "## chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5fde679f-2291-4903-b0ed-8390b7c1a64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 6, 7, 9], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2_matrix_m, pvalue_matrix_m = p_x2(client_models_m, x_test_m)\n",
    "temp4 = meancal(x2_matrix_m)\n",
    "iqrfunc(temp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d8ce4-ac02-4f7a-8c1b-e73293df547f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b479ca-12a8-41e7-9164-cec235a59f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "363a1145-27bd-470d-8184-fe29d1d0aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def combine_matrices(models, x_test):\n",
    "    # دریافت خروجی‌های چهار تابع\n",
    "    delta_class_matrix = delta_class(models, x_test)\n",
    "    delta_score_matrix = delta_score(models, x_test)\n",
    "    ks_matrix, _ = p_ks(models, x_test)  # فقط ks_matrix را می‌گیریم\n",
    "    x2_matrix, _ = p_x2(models, x_test)  # فقط x2_matrix را می‌گیریم\n",
    "\n",
    "    # ترکیب همه ماتریس‌ها\n",
    "    combined_matrix = np.stack([delta_class_matrix, delta_score_matrix, ks_matrix, x2_matrix], axis=2)\n",
    "    \n",
    "    # محاسبه میانه برای هر عنصر\n",
    "    median_matrix = np.median(combined_matrix, axis=2)\n",
    "\n",
    "    return median_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbbf746-68db-4742-83b5-534e9bef4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan(distance_matrix, eps, min_samples):\n",
    "    n = distance_matrix.shape[0]\n",
    "    labels = -1 * np.ones(n)  # -1: برای نقاط outlier\n",
    "    \n",
    "    for i in range(n):\n",
    "        if labels[i] != -1:  # اگر نقطه قبلا برچسب خورده باشد، رد می‌شود\n",
    "            continue\n",
    "        \n",
    "        # پیدا کردن همسایگان نقطه i\n",
    "        neighbors = np.where(distance_matrix[i] <= eps)[0]\n",
    "        \n",
    "        if len(neighbors) < min_samples:\n",
    "            labels[i] = -1  # outlier\n",
    "        else:\n",
    "            labels[i] = 0  # برای نقاطی که outlier نیستند، برچسب 0 گذاشته می‌شود\n",
    "\n",
    "    outliers = np.where(labels == -1)[0]\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e825ea19-4761-4c1d-907e-516079c9d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 6 9]\n"
     ]
    }
   ],
   "source": [
    "def automatic_eps_min_samples(median_matrix, num_clients):\n",
    "    # محاسبه محدوده داده‌ها\n",
    "    min_distance = np.min(median_matrix)\n",
    "    max_distance = np.max(median_matrix)\n",
    "    \n",
    "    # تنظیم eps به عنوان درصدی از محدوده\n",
    "    eps = 0.1 * (max_distance - min_distance)  # می‌توانید درصد را تنظیم کنید\n",
    "\n",
    "    # تنظیم min_samples بر اساس تعداد کلاینت‌ها\n",
    "    min_samples = max(2, num_clients // 2)  # حداقل 2 و حداکثر نصف تعداد کلاینت‌ها\n",
    "\n",
    "    return eps, min_samples\n",
    "\n",
    "# استفاده از تابع\n",
    "median_matrix = combine_matrices(client_models_m, x_test_m)\n",
    "num_clients = len(client_models_m)  # تعداد کلاینت‌ها\n",
    "eps, min_samples = automatic_eps_min_samples(median_matrix, num_clients)\n",
    "median=combine_matrices(client_models_m, x_test_m)\n",
    "outleir=dbscan(median, eps, min_samples)\n",
    "print(outleir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f147127-685c-4664-a195-77c0381df989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.80935), ('loss', 0.60675085)]))])\n",
      "Round 2 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9001333), ('loss', 0.32920736)]))])\n",
      "Round 3 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.92396665), ('loss', 0.25675794)]))])\n",
      "Round 4 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9355), ('loss', 0.22019647)]))])\n",
      "Round 5 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9428333), ('loss', 0.20165376)]))])\n",
      "Round 6 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9482833), ('loss', 0.18610458)]))])\n",
      "Round 7 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.95265), ('loss', 0.17774697)]))])\n",
      "Round 8 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9546667), ('loss', 0.16964802)]))])\n",
      "Round 9 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9566), ('loss', 0.16272749)]))])\n",
      "Round 10 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.9594333), ('loss', 0.15885413)]))])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# بارگذاری داده‌ها\n",
    "(x_train_m, y_train_m), (x_test_m, y_test_m) = mnist.load_data()\n",
    "\n",
    "# نرمال‌سازی داده‌ها\n",
    "x_train_m = x_train_m.astype('float32') / 255.0\n",
    "x_test_m = x_test_m.astype('float32') / 255.0\n",
    "\n",
    "# تغییر شکل داده‌ها برای MLP\n",
    "x_train_m = x_train_m.reshape((x_train_m.shape[0], -1))  # تبدیل به (batch_size, 784)\n",
    "x_test_m = x_test_m.reshape((x_test_m.shape[0], -1))    # تبدیل به (batch_size, 784)\n",
    "\n",
    "# کدگذاری برچسب‌ها\n",
    "y_train_m = tf.keras.utils.to_categorical(y_train_m, 10)\n",
    "y_test_m = tf.keras.utils.to_categorical(y_test_m, 10)\n",
    "\n",
    "# تقسیم داده‌ها بین کلاینت‌ها\n",
    "def split_data(x, y, num_splits):\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split_indices = np.array_split(indices, num_splits)\n",
    "    split_data = [(x[indices], y[indices]) for indices in split_indices]  \n",
    "    return split_data\n",
    "\n",
    "num_models = 10\n",
    "client_data_m = split_data(x_train_m, y_train_m, num_models)\n",
    "\n",
    "# تعریف مدل Keras برای MNIST\n",
    "def create_keras_mnist_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# تبدیل مدل Keras به مدل فدرال TFF\n",
    "def model_fn():\n",
    "    keras_model = create_keras_mnist_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model=keras_model,\n",
    "        input_spec=(tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=[None, 10], dtype=tf.float32)),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "def client_optimizer_fn():\n",
    "    return tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# ساخت فرآیند میانگین‌گیری فدرال با بهینه‌ساز کلاینت‌ها\n",
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=model_fn,\n",
    "    client_optimizer_fn=client_optimizer_fn\n",
    ")\n",
    "\n",
    "# آغاز یادگیری فدرال\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "# اجرای حلقه آموزش فدرال\n",
    "total_rounds = 10\n",
    "round_counter = 0\n",
    "\n",
    "while round_counter < total_rounds:\n",
    "    round_counter += 1\n",
    "    \n",
    "    # Create TensorFlow datasets for each client\n",
    "    federated_data = [\n",
    "        tf.data.Dataset.from_tensor_slices((client_data_m[i][0], client_data_m[i][1]))\n",
    "        .batch(20)\n",
    "        for i in range(num_models)\n",
    "    ]                   \n",
    "    \n",
    "    # انجام یک راند آموزش فدرال\n",
    "    state, metrics = iterative_process.next(state, federated_data)\n",
    "    print(f'Round {round_counter} metrics: {metrics}')\n",
    "\n",
    "    # # اجرای دیباگ هر 3 راند یک‌بار\n",
    "    # if round_counter == 1 or round_counter % 3 == 0:\n",
    "    #     # محاسبه ماتریس فاصله‌ها\n",
    "    #     median_matrix = combine_matrices(client_models_m, x_test_m)\n",
    "\n",
    "    #     # محاسبه eps و min_samples\n",
    "    #     num_clients = len(client_models_m)\n",
    "    #     eps, min_samples = automatic_eps_min_samples(median_matrix, num_clients)\n",
    "\n",
    "    #     # اجرای فرایند DBSCAN برای شناسایی کلاینت‌های مشکل‌دار\n",
    "    #     outliers = dbscan(median_matrix, eps, min_samples)\n",
    "    #     print(f'Outliers after round {round_counter}: {outliers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad35d175-b366-450a-bcc7-196862739cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 metrics: OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('value_sum_process', ()), ('weight_sum_process', ())])), ('train', OrderedDict([('categorical_accuracy', 0.80845), ('loss', 0.6092876)]))])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'combined_process' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 89\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# # اجرای دیباگ هر 3 راند یک‌بار\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# if round_counter == 1 or round_counter % 3 == 0:\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#     # محاسبه ماتریس فاصله‌ها\u001b[39;00m\n\u001b[0;32m     88\u001b[0m num_clients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(client_models_m)  \u001b[38;5;66;03m# تعداد کلاینت‌ها\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m outliers \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_process\u001b[49m(client_models_m, x_test_m, num_clients)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(outliers)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# اجرای فرایند DBSCAN برای شناسایی کلاینت‌های مشکل‌دار\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combined_process' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# بارگذاری داده‌ها\n",
    "(x_train_m, y_train_m), (x_test_m, y_test_m) = mnist.load_data()\n",
    "\n",
    "# نرمال‌سازی داده‌ها\n",
    "x_train_m = x_train_m.astype('float32') / 255.0\n",
    "x_test_m = x_test_m.astype('float32') / 255.0\n",
    "\n",
    "# تغییر شکل داده‌ها برای MLP\n",
    "x_train_m = x_train_m.reshape((x_train_m.shape[0], -1))  # تبدیل به (batch_size, 784)\n",
    "x_test_m = x_test_m.reshape((x_test_m.shape[0], -1))    # تبدیل به (batch_size, 784)\n",
    "\n",
    "# کدگذاری برچسب‌ها\n",
    "y_train_m = tf.keras.utils.to_categorical(y_train_m, 10)\n",
    "y_test_m = tf.keras.utils.to_categorical(y_test_m, 10)\n",
    "\n",
    "# تقسیم داده‌ها بین کلاینت‌ها\n",
    "def split_data(x, y, num_splits):\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    split_indices = np.array_split(indices, num_splits)\n",
    "    split_data = [(x[indices], y[indices]) for indices in split_indices]  \n",
    "    return split_data\n",
    "\n",
    "num_models = 10\n",
    "client_data_m = split_data(x_train_m, y_train_m, num_models)\n",
    "\n",
    "# تعریف مدل Keras برای MNIST\n",
    "def create_keras_mnist_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# تبدیل مدل Keras به مدل فدرال TFF\n",
    "def model_fn():\n",
    "    keras_model = create_keras_mnist_model()\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model=keras_model,\n",
    "        input_spec=(tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=[None, 10], dtype=tf.float32)),\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    "    )\n",
    "\n",
    "def client_optimizer_fn():\n",
    "    return tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# ساخت فرآیند میانگین‌گیری فدرال با بهینه‌ساز کلاینت‌ها\n",
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn=model_fn,\n",
    "    client_optimizer_fn=client_optimizer_fn\n",
    ")\n",
    "\n",
    "# آغاز یادگیری فدرال\n",
    "state = iterative_process.initialize()\n",
    "\n",
    "# اجرای حلقه آموزش فدرال\n",
    "total_rounds = 10\n",
    "round_counter = 0\n",
    "\n",
    "while round_counter < total_rounds:\n",
    "    round_counter += 1\n",
    "    \n",
    "    # Create TensorFlow datasets for each client\n",
    "    federated_data = [\n",
    "        tf.data.Dataset.from_tensor_slices((client_data_m[i][0], client_data_m[i][1]))\n",
    "        .batch(20)\n",
    "        for i in range(num_models)\n",
    "    ]                   \n",
    "    \n",
    "    # انجام یک راند آموزش فدرال\n",
    "    state, metrics = iterative_process.next(state, federated_data)\n",
    "    print(f'Round {round_counter} metrics: {metrics}')\n",
    "\n",
    "    # # اجرای دیباگ هر 3 راند یک‌بار\n",
    "    # if round_counter == 1 or round_counter % 3 == 0:\n",
    "    #     # محاسبه ماتریس فاصله‌ها\n",
    "    num_clients = len(client_models_m)  # تعداد کلاینت‌ها\n",
    "    outliers = combined_process(client_models_m, x_test_m, num_clients)\n",
    "    print(outliers)\n",
    "\n",
    "    # اجرای فرایند DBSCAN برای شناسایی کلاینت‌های مشکل‌دار\n",
    "    outliers = dbscan(median_matrix, eps, min_samples)\n",
    "    print(f'Outliers after round {round_counter}: {outliers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba73ca9-b1f8-42c1-90ce-fb719ca01226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_fed_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
